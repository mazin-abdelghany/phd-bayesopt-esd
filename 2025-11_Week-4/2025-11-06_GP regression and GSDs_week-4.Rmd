---
title: "GSD and GP regression - week 4"
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: false
date: "2025-11-06"
---

```{css, echo=FALSE}
#TOC {
    max-width: fit-content;
    white-space: nowrap;
}
  
div:has(> #TOC) {
    display: flex;
    flex-direction: row-reverse;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load required libraries
library(mvtnorm)
library(ggplot2)
library(data.table)

# set the seed
set.seed(20251029)
```

# GSD simulations

## Triangular boundaries

Correcting the equations for triangular boundaries.

First, pull the GSD simulations code that includes calculations for type I error ($\alpha$) and power.

```{r}
gsd_simulations <- function(n_analyses = 3,
                            upper_bounds = c(2.5, 2, 1.5),
                            lower_bounds = c(0, 0.75, 1.5),
                            n_patients = c(20, 40, 60),
                            null_hypothesis = 0,
                            alt_hypothesis = 0.5,
                            variance = 1) {
  
  # sanity checks
  # sanity checks, function stops
  if(length(upper_bounds) != length(lower_bounds)) {
    stop("Warning: number of upper bounds must equal number of lower bounds")
  }
  
  if(length(n_patients) != length(upper_bounds)) {
    stop("Warning: number of patients vector must equal number of bounds")
  }
  
  if(n_analyses != length(upper_bounds)) {
    stop("Warning: number of analyses must equal number of bounds")
  }
  
  # assign values for null and alt hypotheses
  theta_0 <- null_hypothesis
  delta <- alt_hypothesis
  
  # empty mean vectors to fill
  mean_0 <- c()
  mean_1 <- c()
  
  # need to parse the upper and lower boundaries of the design
  # for futility and efficacy, must put the bounds of integration correctly 
  # for pmvnorm
  futility_l_bounds <- list()
  futility_u_bounds <- list()
  efficacy_l_bounds <- list()
  efficacy_u_bounds <- list()

  n_analyses <- length(upper_bounds)

  for (i in 1:n_analyses) {
    
    # special case of i = 1
    if (i == 1) {
      futility_l_bounds[[i]] <- lower_bounds[i]
      futility_u_bounds[[i]] <- upper_bounds[i]
      efficacy_l_bounds[[i]] <- lower_bounds[i]
      efficacy_u_bounds[[i]] <- upper_bounds[i]
      next
    }
    
    # all other cases
    futility_l_bounds[[i]] <- c(lower_bounds[1:i-1], -Inf)
    futility_u_bounds[[i]] <- c(upper_bounds[1:i-1], lower_bounds[i])
    
    efficacy_l_bounds[[i]] <- c(lower_bounds[1:i-1], upper_bounds[i])
    efficacy_u_bounds[[i]] <- c(upper_bounds[1:i-1], Inf)
  }
  
  # list of probabilities to return
  probs_to_return <- list()
  
  # list of SIGMAs
  SIGMA_list <- list()
    
  for (i in 1:n_analyses) {
    if (i == 1) next
    
    # start with diagonal matrix for SIGMA
    SIGMA <- diag(nrow = i)
    
    # n = 2, need to fill all but 11, 22
    # n = 3, need to fill all but 11, 22, 33
    # n = 4, need to fill all but 11, 22, 33, 44
    # etc. 
    for(i in 1:i) {
      for(j  in 1:i) {
        
        # leave the 1s on the diagonal, skip this iteration of for loop
        if(i == j) next
        
        # when i is less than j, the lower number of patients will be in numerator
        if(i < j) SIGMA[i,j] <- sqrt(n_patients[i] / n_patients[j])
        
        # when i is greater than j, the lower number of patients will be in numerator
        if(i > j) SIGMA[i,j] <- sqrt(n_patients[j] / n_patients[i])
        
      }
    }
    
    SIGMA_list[[i]] <- SIGMA
  }
  
  
  for (i in 1:n_analyses) {
    
    ##############
    # ANALYSIS 1 #
    ##############
    if(i == 1) {
      # mean under null
      mean_0[i] <- theta_0 * sqrt(n_patients[i]/(2*variance))
      
      # mean under alternative
      mean_1[i] <- delta * sqrt(n_patients[i]/(2*variance))
      
      # prob stop for futility, null
      futility_null <- pnorm(futility_l_bounds[[i]], 
                             mean = mean_0, 
                             sd = sqrt(variance))
      
      # prob stop for efficacy, null
      efficacy_null <- pnorm(efficacy_u_bounds[[i]], 
                             mean = mean_0, 
                             sd = sqrt(variance), 
                             lower.tail = FALSE)
  
      # prob stop for futility, alt
      futility_alt <- pnorm(futility_l_bounds[[i]], 
                            mean = mean_1, 
                            sd = sqrt(variance))
      
      # prob stop for efficacy
      efficacy_alt <- pnorm(efficacy_u_bounds[[i]], 
                            mean = mean_1, 
                            sd = sqrt(variance), 
                            lower.tail = FALSE)
      
      probs_to_return[[i]] <- c(futility_null, efficacy_null, futility_alt, efficacy_alt)
      names(probs_to_return[[i]]) <- c("futility_null", "efficacy_null", "futility_alt", "efficacy_alt")
      
      next
    }
    
    ######################
    # ALL OTHER ANALYSES #
    ######################
    
    # next mean under null
    mean_0[i] <- theta_0 * sqrt(n_patients[i] / (2 * variance))
    
    # next mean under alternative
    mean_1[i] <- delta * sqrt(n_patients[i]/ (2*variance))
    
    # bounds for these will be same
    # futility under null
    futility_null <- pmvnorm(lower = futility_l_bounds[[i]], 
                             upper = futility_u_bounds[[i]], 
                             mean = mean_0, corr = SIGMA_list[[i]])
    # futility under alt
    futility_alt <- pmvnorm(lower = futility_l_bounds[[i]], 
                            upper = futility_u_bounds[[i]], 
                            mean = mean_1, corr = SIGMA_list[[i]])
    
    # bounds for these will be same
    # futility under null
    efficacy_null <- pmvnorm(lower = efficacy_l_bounds[[i]], 
                             upper = efficacy_u_bounds[[i]],
                             mean = mean_0, corr = SIGMA_list[[i]])
    # futility under alt
    efficacy_alt <- pmvnorm(lower = efficacy_l_bounds[[i]], 
                            upper = efficacy_u_bounds[[i]], 
                            mean = mean_1, corr = SIGMA_list[[i]])
    
    probs_to_return[[i]] <- c(futility_null, efficacy_null, futility_alt, efficacy_alt)
    names(probs_to_return[[i]]) <- c("futility_null", "efficacy_null", "futility_alt", "efficacy_alt")
    
  }
    
  # vector to collect the sum of futility and efficacy probabilities
  sum_probs <- c()
  
  # get alpha and power
  alpha <- 0
  power <- 0
  
  for (i in 1:n_analyses) {
    
    # pull the probabilities from the list
    tmp_probs <- probs_to_return[[i]]
    
    # gather them into a vector
    # 3:4 because we want to calculate under the alternative
    sum_probs <- c(sum_probs, sum(tmp_probs[3:4]))
    
    alpha <- tmp_probs[2] + alpha
    power <- tmp_probs[4] + power
    
  }
  
  # calculate the expected sample size
  ess <- sum(n_patients * sum_probs)
  
  # add the expected sample size to the list
  return_values <- append(probs_to_return, values = c(ess, alpha, power))
  
  # name the list
  names_for_list <- as.vector(sapply("analysis_", paste0, 1:n_analyses))
  names_for_list <- c(names_for_list, "expected_sample_size", "alpha", "power")
  names(return_values) <- names_for_list
  
  # return probabilities and ESS
  return_values
}
```

There are two mistakes that seem to have been issues:
1. The $1/\texttt{delta_tild}$ term should be squared.
2. The calculation of $\widetilde{delta}$ should be skipped, for now.
3. The individual information calculations do not match the information calculated at maximum and then fractionated over each analysis.

```{r}
triangular_bounds <- function(n_analyses = 3,
                              alpha = 0.05,
                              delta = 0.5) {
  
  I_L_term1 <- (4 * (0.583^2)) / n_analyses
  I_L_term2 <- 8 * log(1/(2*alpha))
  I_L_term3 <- (2 * 0.583) / sqrt(n_analyses)
  
  I_L <- (sqrt(I_L_term1 + I_L_term2) - I_L_term3)^2 * (1 / delta)^2
  
  bounds_term1 <- (2/delta) * log(1/(2*alpha))
  bounds_term2 <- 0.583 * sqrt(I_L / n_analyses)
  
  analysis_fracs <- (1:n_analyses / n_analyses)
  
  I_L_fracs <- I_L * analysis_fracs
  
  e_l <- (bounds_term1 - bounds_term2 + ((0.25*delta) * analysis_fracs * I_L )) / sqrt(I_L_fracs)

  f_l <- (-bounds_term1 + bounds_term2 + ((0.75*delta) * analysis_fracs * I_L )) / sqrt(I_L_fracs)
  
  return(list(upper = e_l, lower = f_l, info = I_L_fracs))
  
}
```

Quick assessment of the characteristics of the calculated boundaries.

```{r}
delta <- 0.5

bounds <- triangular_bounds(n_analyses = 3,
                            alpha = 0.05,
                            delta = delta)

gsd_simulations(n_analyses = 3,
                upper_bounds = bounds$upper,
                lower_bounds = bounds$lower,
                alt_hypothesis = delta,
                n_patients = c(20,40,60),
                variance = 1)
```

Dr. Robertson's code for triangular bounds.

```{r}
# type I error
alpha <- 0.05

# number of analyses
K <- 3

# delta, true assumed clinically relevant difference
delta <- 0.5

Imax = (sqrt(4*0.583^2/K + 8*log(1/(2*alpha))) - 2*0.583/sqrt(K))^2/delta^2


# Score statistic

dk = (2/delta)*log(1/(2*alpha)) - 0.583*sqrt(Imax/K) + (delta/4)*((1:K)/K)*Imax
ck = -(2/delta)*log(1/(2*alpha)) + 0.583*sqrt(Imax/K) + (3*delta/4)*((1:K)/K)*Imax


ak = dk/sqrt(Imax*(1:K)/K)
bk = ck/sqrt(Imax*(1:K)/K)

ggplot() +
  geom_line(mapping = aes(x = 1:K, y = bk)) +
  geom_line(mapping = aes(x = 1:K, y = ak)) +
  geom_line(mapping = aes(x = 1:K, y = ck), color="red") +
  geom_line(mapping = aes(x = 1:K, y = dk), color="red") 
```

## Other boundaries

According to the paper, the symmetric boundaries are calculated using the following formula:

$$
r_l = C_{WT}\left(\frac{l}{L}\right)^{\Omega−0.5}
$$
Pocock (1977) and O'Brien and Fleming (1979) boundaries take $\Omega = 0.5$ or $\Omega = 0$, respectively. Further, a numerical search is used for any chosen $\Omega$ to determine the value of $C_{WT}$ that implies the correct type I error rate $\alpha$. A further search is then used to ascertain the required sample size for the power constraint.

### Pocock

Boundaries given by:

$$
r_l = C_{WT}\left(\frac{l}{L}\right)^{0.5−0.5} = C_{WT}
$$

Using our `gsd_simulations` function, we can perform this numerical search.

To perform the search as coded below, we need a function that rounds down to the nearest power of ten.

```{r}
round_down10 <- function(x) 10^floor(log10(x))
```


```{r}
n_analyses <- 3
alpha <- 0.05
beta <- 0.1

# epsilon must be multiple of ten
epsilon <- 0.00001

# hyperparameter tuned to get close to alpha needed
z_mod <- -0.001

initial_boundaries <- rep_len(qnorm(alpha/n_analyses), length.out = n_analyses)

pk_upper_bounds <- -initial_boundaries
pk_lower_bounds <- initial_boundaries

sim_vals <- gsd_simulations(n_analyses = n_analyses, 
                            upper_bounds = pk_upper_bounds,
                            lower_bounds = pk_lower_bounds,
                            n_patients = c(20,40,60))

while( round_down10(abs(sim_vals$alpha - alpha)) != epsilon){
  
  if ( round_down10(abs(sim_vals$alpha - alpha)) > epsilon) {
    initial_boundaries <- initial_boundaries - z_mod
    
    pk_upper_bounds <- -initial_boundaries
    pk_lower_bounds <- initial_boundaries
    
  } else if ( round_down10(abs(sim_vals$alpha - alpha)) < epsilon) {
    initial_boundaries <- initial_boundaries + z_mod
    
    pk_upper_bounds <- -initial_boundaries
    pk_lower_bounds <- initial_boundaries
    
  }
  
  sim_vals <- gsd_simulations(n_analyses = n_analyses, 
                            upper_bounds = pk_upper_bounds,
                            lower_bounds = pk_lower_bounds,
                            n_patients = c(20,40,60))
  
}

print(list(pk_upper_bounds, pk_lower_bounds, sim_vals$alpha))
```


### O'Brien-Fleming

$$
r_l = C_{WT}\left(\frac{l}{L}\right)^{0−0.5} = C_{WT}\left(\frac{l}{L}\right)^{−0.5}
$$

# GP regression

```{r}
# load required libraries
library(gplite)
library(plotly)
```

## Change restarts in `gp_optim`

Generate data for the function to which the model will be fit.

```{r}
n <- 5
x  <- matrix(seq(-2, 2, length.out = n), ncol = 1)
y  <- x^2
```

Plot the data.

```{r}
ggplot() + 
  geom_point(aes(x=x, y=y), size=2) +
  xlab('x') + ylab('y') 
```


Initialize the GP model.

```{r}
# Specify the GP model we want to use:
gp_empty <- gp_init(
  
  # A squared exponential (aka Gaussian aka RBF) kernel
  cfs = cf_sexp(
    vars = NULL,
    lscale = 0.3,
    magn = 1,
    prior_lscale = prior_logunif(),
    prior_magn = prior_logunif(),
    normalize = FALSE
  ),  
  
  # Assume Gaussian distributed errors
  lik = lik_gaussian(
    sigma = 0.5, 
    prior_sigma = prior_logunif()
  ), 
  
  # Use the full covariance (i.e., do not approximate)
  method = method_full() 
  
)
```

Optimize the model using the internal function.

```{r}
# Now fit the model to the data:
gp_optimized <- gp_optim(gp_empty, x, y, verbose = FALSE)
```

Plot the model.

```{r}
# compute the predictive mean and variance in a grid of points
xt   <- seq(-4, 4, len=150)
pred <- gp_pred(gp_optimized, xt, var = T)

# visualize
mu <- pred$mean
lb <- pred$mean - 2*sqrt(pred$var)
ub <- pred$mean + 2*sqrt(pred$var)

ggplot() + 
  geom_ribbon(aes(x=xt, ymin=lb, ymax=ub), fill='lightgray') +
  geom_line(aes(x=xt, y=mu), linewidth = 0.5) +
  geom_point(aes(x=x, y=y), size=2) +
  xlab('x') + ylab('y')
```

Pull the energy.

```{r}
gp_energy(gp_optimized)
```

Increase the number of restarts.

```{r}
# Now fit the model to the data:
gp_optimized_10 <- gp_optim(
  
  gp = gp_empty, 
  x = x, 
  y = y, 
  restarts = 10,
  verbose = FALSE
  
)
```

Plot the model with larger number of restarts.

```{r}
# compute the predictive mean and variance in a grid of points
pred <- gp_pred(gp_optimized_10, xt, var = T)

# visualize
mu <- pred$mean
lb <- pred$mean - 2*sqrt(pred$var)
ub <- pred$mean + 2*sqrt(pred$var)

ggplot() + 
  geom_ribbon(aes(x=xt, ymin=lb, ymax=ub), fill='lightgray') +
  geom_line(aes(x=xt, y=mu), linewidth = 0.5) +
  geom_point(aes(x=x, y=y), size=2) +
  xlab('x') + ylab('y')
```

Pull the energy.

```{r}
gp_energy(gp_optimized_10)
```

Increase to 1000 restarts.

```{r}
# Now fit the model to the data:
gp_optimized_1000 <- gp_optim(
  
  gp = gp_empty, 
  x = x, 
  y = y, 
  restarts = 1000,
  verbose = FALSE
  
)
```

Plot the model with larger number of restarts.

```{r}
# compute the predictive mean and variance in a grid of points
pred <- gp_pred(gp_optimized_1000, xt, var = T)

# visualize
mu <- pred$mean
lb <- pred$mean - 2*sqrt(pred$var)
ub <- pred$mean + 2*sqrt(pred$var)

ggplot() + 
  geom_ribbon(aes(x=xt, ymin=lb, ymax=ub), fill='lightgray') +
  geom_line(aes(x=xt, y=mu), linewidth = 0.5) +
  geom_point(aes(x=x, y=y), size=2) +
  xlab('x') + ylab('y')
```

Pull the energy.

```{r}
gp_energy(gp_optimized_1000)
```

Increase to 1,000,000 restarts.

```{r}
# Now fit the model to the data:
gp_optimized_1e6 <- gp_optim(
  
  gp = gp_empty, 
  x = x, 
  y = y, 
  restarts = 1e6,
  verbose = FALSE
  
)
```

Plot the model with larger number of restarts.

```{r}
# compute the predictive mean and variance in a grid of points
pred <- gp_pred(gp_optimized_1e6, xt, var = T)

# visualize
mu <- pred$mean
lb <- pred$mean - 2*sqrt(pred$var)
ub <- pred$mean + 2*sqrt(pred$var)

ggplot() + 
  geom_ribbon(aes(x=xt, ymin=lb, ymax=ub), fill='lightgray') +
  geom_line(aes(x=xt, y=mu), linewidth = 0.5) +
  geom_point(aes(x=x, y=y), size=2) +
  xlab('x') + ylab('y')
```

Pull the energy.

```{r}
gp_energy(gp_optimized_1e6)
```

## Marginal likelihood surface; more points

### Original surface plot.

```{r}
ell <- seq(0.5, 3, by = 0.05)
  
sigma_f <- seq(2, 8, by = 0.05)

hyperparams <- expand.grid(
  ell = ell,
  sigma_f = sigma_f
)

# collect energies in the empty vector
energy <- c()

# run 6000+ models
for (i in 1:dim(hyperparams)[1]) {
  
  gp_empty <- gp_init(
    # A squared exponential (aka Gaussian aka RBF) kernel
    cfs = cf_sexp(
      vars = NULL,
      lscale = hyperparams$ell[i],
      magn = hyperparams$sigma_f[i],
      prior_lscale = prior_logunif(),
      prior_magn = prior_logunif(),
      normalize = FALSE
    ),  
    
    # Assume Gaussian distributed errors
    lik = lik_gaussian(
      sigma = 0.01, 
      prior_sigma = prior_logunif()
    ), 
    
    # Use the full covariance (i.e., do not approximate)
    method = method_full() 
  )
  
  gp_raw <- gp_fit(gp_empty, x, y)
  
  energy[i] <- gp_energy(gp_raw)
  
}

surface_plot <- cbind(hyperparams, energy)

surface_plot[which.min(surface_plot$energy),]
```

Model with $\ell = 2$ and $\sigma_f^2 = 6$, near the optimal values. Recall that we are setting $\sigma_n^2 = 0.01$.

```{r}
# Specify the GP model we want to use,
# we will specify the lscale and magn using values from above
gp_empty <- gp_init(
  # A squared exponential (aka Gaussian aka RBF) kernel
  cfs = cf_sexp(
    vars = NULL,
    lscale = 2,
    magn = 6,
    prior_lscale = prior_logunif(),
    prior_magn = prior_logunif(),
    normalize = FALSE
  ),  
  
  # Assume Gaussian distributed errors
  lik = lik_gaussian(
    sigma = 0.01, 
    prior_sigma = prior_logunif()
  ), 
  
  # Use the full covariance (i.e., do not approximate)
  method = method_full() 
)

gp_raw_ell2sigma6 <- gp_fit(gp_empty, x, y)
```

Plot the model.

```{r}
# compute the predictive mean and variance in a grid of points
pred_ell2sigma6 <- gp_pred(gp_raw_ell2sigma6, xt, var = T)

# visualize
mu_ell2sigma6 <- pred_ell2sigma6$mean
lb_ell2sigma6 <- pred_ell2sigma6$mean - 2*sqrt(pred_ell2sigma6$var)
ub_ell2sigma6 <- pred_ell2sigma6$mean + 2*sqrt(pred_ell2sigma6$var)

ggplot() + 
  geom_ribbon(aes(x=xt, ymin=lb_ell2sigma6, ymax=ub_ell2sigma6), fill='lightgray') +
  geom_line(aes(x=xt, y=mu_ell2sigma6), linewidth = 0.5) +
  geom_point(aes(x=x, y=y), size=2) +
  xlab('x') + ylab('y')
```

Plot the entire surface.

```{r}
fig <- plot_ly() %>% 
  
  add_trace(
    x = unique(surface_plot$sigma_f), 
    y = unique(surface_plot$ell), 
    z = matrix(surface_plot$energy, nrow = 51, ncol = 121),
    type = "surface") %>% 
  
  add_trace(
    x = 6.1,
    y = 1.95,
    z = 11.05812,
    type = "scatter3d",
    mode = "markers"
  ) %>%
  
  layout(
    scene = list(
      aspectmode = 'manual',  # Set to manual for custom aspect ratio
      aspectratio = list(x = 1, y = 1, z = 0.5),  # Adjust x, y, z ratios
      
      xaxis = list(
        title = "signal variance"
      ),
      
      yaxis = list(
        title = "length-scale parameter"
      ),
      
      zaxis = list(
        title = "-log(marginal likelihood)"
      )
    )
  )
  
fig
```

Plot the surface nearest the optimum value.

```{r}
fig <- plot_ly() %>% 
  
  add_trace(
    x = unique(surface_plot$sigma_f)[80:121], 
    y = unique(surface_plot$ell)[10:51], 
    z = matrix(surface_plot$energy, nrow = 51, ncol = 121)[10:51, 80:121],
    type = "surface") %>% 
  
  add_trace(
    x = 6.1,
    y = 1.95,
    z = 11.05812,
    type = "scatter3d",
    mode = "markers"
  ) %>%
  
  layout(
    scene = list(
      aspectmode = 'manual',  # Set to manual for custom aspect ratio
      aspectratio = list(x = 1, y = 1, z = 0.5),  # Adjust x, y, z ratios
      
      xaxis = list(
        title = "signal variance"
      ),
      
      yaxis = list(
        title = "length-scale parameter"
      ),
      
      zaxis = list(
        title = "-log(marginal likelihood)"
      )
    )
  )
  
fig
```

### More points!

Next, increase the number of points that we are fitting, but within the originally specified range of the function $x \in (-2, 2)$. Below, I have added 3 new points. 

```{r}
# n is 7 now; used to be 5
n_new <- 7
x_new  <- matrix(seq(-2, 2, length.out = n_new), ncol = 1)
y_new  <- x_new^2
```

Plot the data.

```{r}
ggplot() + 
  geom_point(aes(x = x_new, y = y_new), size=2) +
  xlab('x') + ylab('y') 
```

Now, fit the model using `gp_optim`.

```{r}
gp_empty_7 <- gp_init(
  
  # A squared exponential (aka Gaussian aka RBF) kernel
  cfs = cf_sexp(
    vars = NULL,
    lscale = 0.3,
    magn = 1,
    prior_lscale = prior_logunif(),
    prior_magn = prior_logunif(),
    normalize = FALSE
  ),  
  
  # Assume Gaussian distributed errors
  lik = lik_gaussian(
    sigma = 0.5, 
    prior_sigma = prior_logunif()
  ), 
  
  # Use the full covariance (i.e., do not approximate)
  method = method_full() 
  
)
```

Optimize.

```{r}
gp_optimized_7 <- gp_optim(gp = gp_empty_7,
                           x = x_new,
                           y = y_new,
                           restarts = 5,
                           verbose = TRUE)
```

Optimal value of $\sigma_f^2 = 7.24$ and optimal value of $\ell=3.8$.

Pull the energy.

```{r}
gp_energy(gp_optimized_7)
```


Plot the optimal model.

```{r}
xt_more <- seq(-10, 10, len=150)
pred_7 <- gp_pred(gp = gp_optimized_7,
                  xnew = xt_more,
                  var = TRUE)

mu_pred_7 <- pred_7$mean
lb_pred_7 <- pred_7$mean - 2*sqrt(pred_7$var)
ub_pred_7 <- pred_7$mean + 2*sqrt(pred_7$var)

ggplot() + 
  geom_ribbon(aes(x=xt_more, ymin=lb_pred_7, ymax=ub_pred_7), fill='lightgray') +
  geom_line(aes(x=xt_more, y=mu_pred_7), linewidth = 0.5) +
  geom_point(aes(x=x_new, y=y_new), size=2) +
  xlab('x') + ylab('y')

```

### Generate the surface

Recall, optimal value of $\sigma_f^2 = 7.24$ and optimal value of $\ell=3.8$. We will set $\sigma_n^2 = 0.01$.

```{r}
# using prior optimization, set sequence
ell <- seq(1, 6, by = 0.05)

# using prior optimization, set sequence
sigma_f <- seq(4, 10, by = 0.05)

hyperparams <- expand.grid(
  ell = ell,
  sigma_f = sigma_f
)

# collect energies in the empty vector
energy <- vector(mode = "numeric", length = dim(hyperparams)[1])

# run 6000+ models
for (i in 1:dim(hyperparams)[1]) {
  
  gp_empty <- gp_init(
    # A squared exponential (aka Gaussian aka RBF) kernel
    cfs = cf_sexp(
      vars = NULL,
      lscale = hyperparams$ell[i],
      magn = hyperparams$sigma_f[i],
      prior_lscale = prior_logunif(),
      prior_magn = prior_logunif(),
      normalize = FALSE
    ),  
    
    # Assume Gaussian distributed errors
    lik = lik_gaussian(
      sigma = 0.01, 
      prior_sigma = prior_logunif()
    ), 
    
    # Use the full covariance (i.e., do not approximate)
    method = method_full() 
  )
  
  gp_raw <- gp_fit(gp_empty, x_new, y_new)
  
  energy[i] <- gp_energy(gp_raw)
  
}

surface_plot <- cbind(hyperparams, energy)

surface_plot[which.min(surface_plot$energy),]
```

Is the range of energies lower value?

```{r}
range(surface_plot$energy)
```

Plot the surface.

```{r}
fig <- plot_ly() %>% 
  
  add_trace(
    x = unique(surface_plot$sigma_f), 
    y = unique(surface_plot$ell), 
    z = matrix(surface_plot$energy, nrow = 101, ncol = 121),
    type = "surface") %>% 
  
  add_trace(
    x = 10,
    y = 2.9,
    z = 5.433,
    type = "scatter3d",
    mode = "markers"
  ) %>%
  
  layout(
    scene = list(
      aspectmode = 'manual',  # Set to manual for custom aspect ratio
      aspectratio = list(x = 1, y = 1, z = 0.5),  # Adjust x, y, z ratios
      
      xaxis = list(
        title = "signal variance"
      ),
      
      yaxis = list(
        title = "length-scale parameter"
      ),
      
      zaxis = list(
        title = "-log(marginal likelihood)"
      )
    )
  )
  
fig
```


Plot the surface with lowest energies.

```{r}
fig <- plot_ly() %>% 
  
  add_trace(
    x = unique(surface_plot$sigma_f), 
    y = unique(surface_plot$ell)[1:45], 
    z = matrix(surface_plot$energy, nrow = 101, ncol = 121)[1:45, ],
    type = "surface") %>% 
  
  add_trace(
    x = 10,
    y = 2.9,
    z = 5.433,
    type = "scatter3d",
    mode = "markers"
  ) %>%
  
  layout(
    scene = list(
      aspectmode = 'manual',  # Set to manual for custom aspect ratio
      aspectratio = list(x = 1, y = 1, z = 0.5),  # Adjust x, y, z ratios
      
      xaxis = list(
        title = "signal variance"
      ),
      
      yaxis = list(
        title = "length-scale parameter"
      ),
      
      zaxis = list(
        title = "-log(marginal likelihood)"
      )
    )
  )
  
fig
```

## Broken optimization

$n=7$ works, but $n=8$ or $n=9$ does not.

```{r}
# n is 8 now
n_new <- 8
x_new  <- matrix(seq(-2, 2, length.out = n_new), ncol = 1)
y_new  <- x_new^2
```

Plot the data.

```{r}
ggplot() + 
  geom_point(aes(x = x_new, y = y_new), size=2) +
  xlab('x') + ylab('y') 
```

Now, fit the model using `gp_optim`.

```{r}
gp_empty_8 <- gp_init(
  
  # A squared exponential (aka Gaussian aka RBF) kernel
  cfs = cf_sexp(
    vars = NULL,
    lscale = 0.3,
    magn = 1,
    prior_lscale = prior_logunif(),
    prior_magn = prior_logunif(),
    normalize = FALSE
  ),  
  
  # Assume Gaussian distributed errors
  lik = lik_gaussian(
    sigma = 0.5, 
    prior_sigma = prior_logunif()
  ), 
  
  # Use the full covariance (i.e., do not approximate)
  method = method_full() 
  
)
```

Optimize.

```{r, error=TRUE}
gp_optimized_8 <- gp_optim(gp = gp_empty_8,
                           x = x_new,
                           y = y_new,
                           restarts = 5,
                           verbose = FALSE)
```