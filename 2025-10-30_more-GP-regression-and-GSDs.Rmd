---
title: "GSD and GP regression - week 3"
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: false
date: "2025-10-30"
---

```{css, echo=FALSE}
#TOC {
    max-width: fit-content;
    white-space: nowrap;
}
  
div:has(> #TOC) {
    display: flex;
    flex-direction: row-reverse;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load required libraries
library(mvtnorm)
library(ggplot2)
library(data.table)

# set the seed
set.seed(20251029)
```

# GSD simulations

Last week, we simulated a 3-stage group sequential design using `for` loops and entry into the next stage of the trial using `if` statements. This method---while easy to understand---may have lower computational efficiency.

Here, we implement the same group sequential design (GSD) using vectorized computations.

First, a reminder of the GSD parameters:

* Number of analyses: 3
* Number of patients per group: $\mathbf{n} = (20, 40, 60)$
* Upper bound stopping: $\mathbf{u} = (2.5, 2, 1.5)$
* Lower bound stopping: $\boldsymbol{\ell} = (0, 0.75, 1.5)$
* Number of patients at each analysis: 20
* Null hypothesis: $\theta_0 = 0$
* Alternative hypothesis: $\theta_1 = \delta = 0.5$
* Known variance: $\sigma^2 = 1$

Probabilities for the above are as follows:

|          | $\theta = 0$           |                        | $\theta = 0.5$         |                        |
|----------|------------------------|------------------------|------------------------|------------------------|
| Analysis | Prob stop for futility | Prob stop for efficacy | Prob stop for futility | Prob stop for efficacy |
| 1        | 0.500                  | 0.006                  | 0.057                  | 0.179                  |
| 2        | 0.299                  | 0.019                  | 0.042                  | 0.420                  |
| 3        | 0.137                  | 0.038                  | 0.049                  | 0.253                  |


## Vectorized

First, let us simulate analyses **under the null**.

```{r}
theta <- 0
sigma2 <- 1

matrix_x1 <- replicate(100000, rnorm(60))
matrix_x2 <- replicate(100000, rnorm(60))

diff_matrix <- matrix_x1 - matrix_x2

# analysis 1
# take only first 20 rows
mean_diff_matrix_20 <- colMeans(diff_matrix[1:20,])

z_matrix_20 <- mean_diff_matrix_20 * sqrt( 20 / (2*sigma2) )

mean(z_matrix_20 <= 0)
mean(z_matrix_20 >= 2.5)

# analysis 2
# includes people who moved to analysis 2 only
# which columns moved to analysis 2
moved_to_analysis2 <- z_matrix_20 > 0 & z_matrix_20 < 2.5

# select those columns
diff_matrix_40 <- diff_matrix[, moved_to_analysis2]

# column mean of these
# take only the first 40 rows
mean_diff_matrix_40 <- colMeans(diff_matrix_40[1:40,])

z_matrix_40 <- mean_diff_matrix_40 * sqrt( 40 / (2*sigma2) )

mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 <= 0.75)
mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 >= 2)

# analysis 3
# includes people who moved to analysis 2 then 3 only
# which columns moved to analysis 3
moved_to_analysis3 <- z_matrix_40 > 0.75 & z_matrix_40 < 2

# select those columns from those who moved to anlaysis 2
# therefore, filter the difference matrix of those who moved to analysis 
diff_matrix_60 <- diff_matrix_40[, moved_to_analysis3]

# column mean of these
# take all 60 rows
mean_diff_matrix_60 <- colMeans(diff_matrix_60)

z_matrix_60 <- mean_diff_matrix_60 * sqrt( 60 / (2*sigma2) )

mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 > 0.75 & z_matrix_40 < 2) * mean(z_matrix_60 < 1.5)
mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 > 0.75 & z_matrix_40 < 2) * mean(z_matrix_60 > 1.5)
```


Next, we simulate analyses **under the alternative**.

```{r}
theta <- 0.5
sigma2 <- 1

matrix_x1 <- replicate(100000, rnorm(60, mean = theta))
matrix_x2 <- replicate(100000, rnorm(60))

diff_matrix <- matrix_x1 - matrix_x2

# analysis 1
# take only first 20 rows
mean_diff_matrix_20 <- colMeans(diff_matrix[1:20,])

z_matrix_20 <- mean_diff_matrix_20 * sqrt( 20 / (2*sigma2) )

mean(z_matrix_20 <= 0)
mean(z_matrix_20 >= 2.5)

# analysis 2
# includes people who moved to analysis 2 only
# which columns moved to analysis 2
moved_to_analysis2 <- z_matrix_20 > 0 & z_matrix_20 < 2.5

# select those columns
diff_matrix_40 <- diff_matrix[, moved_to_analysis2]

# column mean of these
# take only the first 40 rows
mean_diff_matrix_40 <- colMeans(diff_matrix_40[1:40,])

z_matrix_40 <- mean_diff_matrix_40 * sqrt( 40 / (2*sigma2) )

mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 <= 0.75)
mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 >= 2)

# analysis 3
# includes people who moved to analysis 2 then 3 only
# which columns moved to analysis 3
moved_to_analysis3 <- z_matrix_40 > 0.75 & z_matrix_40 < 2

# select those columns from those who moved to anlaysis 2
# therefore, filter the difference matrix of those who moved to analysis 
diff_matrix_60 <- diff_matrix_40[, moved_to_analysis3]

# column mean of these
# take all 60 rows
mean_diff_matrix_60 <- colMeans(diff_matrix_60)

z_matrix_60 <- mean_diff_matrix_60 * sqrt( 60 / (2*sigma2) )

mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 > 0.75 & z_matrix_40 < 2) * mean(z_matrix_60 < 1.5)
mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 > 0.75 & z_matrix_40 < 2) * mean(z_matrix_60 > 1.5)
```

### Which is faster?

Last week's implementation, wrapped in a function:

```{r}
for_loops <- function() {

  theta <- 0
  sigma2 <- 1
  
  z_vec <- vector(mode = "numeric", length = 100000)
  z2_vec <- vector(mode = "numeric")
  z3_vec <- vector(mode = "numeric")
  
  analysis2_count <- 0
  analysis3_count <- 0
  
  for (i in 1:100000) {
    x1 <- rnorm(20)
    x2 <- rnorm(20)
  
    mean.diff <- mean(x1 - x2)
  
    z <- mean.diff * sqrt( 20 / (2*sigma2) )
    
    z_vec[i] <-  z
    
    if ( (z > 0) & (z < 2.5) ) {
      
      analysis2_count <- analysis2_count + 1
      
      x3 <- rnorm(20)
      x4 <- rnorm(20)
      
      x1 <- c(x1, x3)
      x2 <- c(x2, x4)
      
      mean.diff <- mean(x1 - x2)
      
      z2 <- mean.diff * sqrt( 40 / (2*sigma2) )
      
      z2_vec[analysis2_count] <- z2
      
      if ( (z2 > 0.75) & (z2 < 2) ) {
        
        analysis3_count <- analysis3_count + 1
        
        x5 <- rnorm(20)
        x6 <- rnorm(20)
        
        x1 <- append(x1, x5)
        x2 <- append(x2, x6)
      
        mean.diff <- mean(x1 - x2)
      
        z3 <- mean.diff * sqrt( 60 / (2*sigma2) )
      
        z3_vec[analysis3_count] <- z3
        
      }
      
    }
    
  }
  
  probs <- c(
    mean(z_vec <= 0),
    mean(z_vec > 0 & z_vec < 2.5) * mean(z2_vec <= 0.75),
    mean(z_vec > 0 & z_vec < 2.5) * mean(z2_vec > 0.75 & z2_vec < 2) * mean(z3_vec < 1.5),
    mean(z_vec > 2.5),
    mean(z_vec > 0 & z_vec < 2.5) * mean(z2_vec > 2),
    mean(z_vec > 0 & z_vec < 2.5) * mean(z2_vec > 0.75 & z2_vec < 2) * mean(z3_vec > 1.5)
  )
  
  probs
  
}
```

This week's implementation, wrapped in a function:

```{r}
vectorized <- function() {
  
  theta <- 0
  sigma2 <- 1
  
  matrix_x1 <- replicate(100000, rnorm(60))
  matrix_x2 <- replicate(100000, rnorm(60))
  
  diff_matrix <- matrix_x1 - matrix_x2
  
  # analysis 1
  # take only first 20 rows
  mean_diff_matrix_20 <- colMeans(diff_matrix[1:20,])
  
  z_matrix_20 <- mean_diff_matrix_20 * sqrt( 20 / (2*sigma2) )
  
  # analysis 2
  # includes people who moved to analysis 2 only
  # which columns moved to analysis 2
  moved_to_analysis2 <- z_matrix_20 > 0 & z_matrix_20 < 2.5
  
  # select those columns
  diff_matrix_40 <- diff_matrix[, moved_to_analysis2]
  
  # column mean of these
  # take only the first 40 rows
  mean_diff_matrix_40 <- colMeans(diff_matrix_40[1:40,])
  
  z_matrix_40 <- mean_diff_matrix_40 * sqrt( 40 / (2*sigma2) )
  
  # analysis 3
  # includes people who moved to analysis 2 then 3 only
  # which columns moved to analysis 3
  moved_to_analysis3 <- z_matrix_40 > 0.75 & z_matrix_40 < 2
  
  # select those columns from those who moved to anlaysis 2
  # therefore, filter the difference matrix of those who moved to analysis 
  diff_matrix_60 <- diff_matrix_40[, moved_to_analysis3]
  
  # column mean of these
  # take all 60 rows
  mean_diff_matrix_60 <- colMeans(diff_matrix_60)
  
  z_matrix_60 <- mean_diff_matrix_60 * sqrt( 60 / (2*sigma2) )
  
  probs <- c(
    mean(z_matrix_20 <= 0),
    mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 <= 0.75),
    mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 > 0.75 & z_matrix_40 < 2) * mean(z_matrix_60 < 1.5),
    mean(z_matrix_20 >= 2.5),
    mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 >= 2),
    mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 > 0.75 & z_matrix_40 < 2) * mean(z_matrix_60 > 1.5)
  )
  
  probs
  
}
```

```{r}
for_loops()
vectorized()
```

```{r, cache=TRUE}
bench::mark(
  for_loops(),
  vectorized(),
  check = FALSE
)
```

| function   | min   | median | mem_alloc |
|------------|-------|--------|-----------|
| for_loops  | 917ms |  917ms |     183MB |
| vectorized | 652ms |  652ms |     414MB |

Add `data.table` to the vectorized implementation:

```{r}
vectorized_DT <- function() {
  
  theta <- 0
  sigma2 <- 1
  
  matrix_x1 <- replicate(100000, rnorm(60))
  matrix_x2 <- replicate(100000, rnorm(60))
  
  matrix_x1 <- as.data.frame(matrix_x1)
  matrix_x2 <- as.data.frame(matrix_x2)
  
  setDT(matrix_x1)
  setDT(matrix_x2)
  
  diff_matrix <- matrix_x1 - matrix_x2
  
  # analysis 1
  # take only first 20 rows
  mean_diff_matrix_20 <- diff_matrix[1:20, lapply(.SD, mean)]
  
  z_matrix_20 <- mean_diff_matrix_20 * sqrt( 20 / (2*sigma2) )
  
  # analysis 2
  # includes people who moved to analysis 2 only
  # which columns moved to analysis 2
  moved_to_analysis2 <- z_matrix_20 > 0 & z_matrix_20 < 2.5
  
  # select those columns
  diff_matrix_40 <- diff_matrix[, ..moved_to_analysis2]
  
  # column mean of these
  # take only the first 40 rows
  mean_diff_matrix_40 <- diff_matrix_40[1:40, lapply(.SD, mean)]
  
  z_matrix_40 <- mean_diff_matrix_40 * sqrt( 40 / (2*sigma2) )
  
  # analysis 3
  # includes people who moved to analysis 2 then 3 only
  # which columns moved to analysis 3
  moved_to_analysis3 <- z_matrix_40 > 0.75 & z_matrix_40 < 2
  
  # select those columns from those who moved to anlaysis 2
  # therefore, filter the difference matrix of those who moved to analysis 
  diff_matrix_60 <- diff_matrix_40[, ..moved_to_analysis3]
  
  # column mean of these
  # take all 60 rows
  mean_diff_matrix_60 <- diff_matrix_60[, lapply(.SD, mean)]
  
  z_matrix_60 <- mean_diff_matrix_60 * sqrt( 60 / (2*sigma2) )
  
  probs <- c(
    mean(z_matrix_20 <= 0),
    mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 <= 0.75),
    mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 > 0.75 & z_matrix_40 < 2) * mean(z_matrix_60 < 1.5),
    mean(z_matrix_20 >= 2.5),
    mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 >= 2),
    mean(z_matrix_20 > 0 & z_matrix_20 < 2.5) * mean(z_matrix_40 > 0.75 & z_matrix_40 < 2) * mean(z_matrix_60 > 1.5)
  )
  
  probs
  
}
```


```{r, cache=TRUE}
# vectorized_DT()
```

The vectorized implementation is about 60% faster but requires 2.25 times as much memory. This is likely because there are fewer for loops in the vectorized implementation, but the matrices for calculation the random normal vectors and $z$ scores are large and must remain in memory.

## GSD function without for loops

In order to perform the function call without for loops, first, assess if the `pmvnorm` function gives values that make sense for multidimensional integrals with infinities as both upper and lower bounds. For example, calculating the stopping probability for efficacy under the null for the first analysis:

$$
\int_{u_1}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \phi_2 \left( 
\begin{bmatrix}
    y_1 & y_2 & y_3
\end{bmatrix}, 
\begin{bmatrix} 
\theta \sqrt{\frac{n_1}{2\sigma^2}} & \theta \sqrt{\frac{n_2}{2\sigma^2}} & \theta \sqrt{\frac{n_3}{2\sigma^2}} 
\end{bmatrix}, 
\begin{bmatrix} 
1 & \sqrt{\frac{n_1}{n_2}} & \sqrt{\frac{n_1}{n_3}}\\ 
\sqrt{\frac{n_1}{n_2}} & 1 & \sqrt{\frac{n_2}{n_3}}\\
\sqrt{\frac{n_1}{n_3}} & \sqrt{\frac{n_2}{n_3}} & 1
\end{bmatrix} 
\right) dy_3\,dy_2\,dy_1
$$

We can try this with code for the probability of stopping for efficacy **under the null** at analysis 1 as a proof of concept.

```{r}
# step up the values needed
# assumed mean and variance
theta_0 <- 0
theta_1 <- 0.5
sigma <- 1

# sample sizes
n1 <- 20
n2 <- 40
n3 <- 60

# bounds of integration
u1 <- 2.5
u2 <- 2
u3 <- 1.5
l1 <- 0
l2 <- 0.75
l3 <- 1.5
```

Perform the integration.

```{r}
# mean vector
mean_0 <- c(theta_0 * sqrt(n1/ (2*sigma) ),
            theta_0 * sqrt(n2/ (2*sigma) ),
            theta_0 * sqrt(n3/ (2*sigma) ))

# covariance matrix
SIGMA <- matrix(
  c(1, sqrt(n1/n2), sqrt(n1/n3),
    sqrt(n1/n2), 1, sqrt(n2/n3),
    sqrt(n1/n3), sqrt(n2/n3), 1),
  nrow = 3
)

lower <- c(u1, -Inf, -Inf)
upper <- c(Inf, Inf, Inf)

pmvnorm(lower = lower, upper = upper, mean = mean_0, corr = SIGMA)
```

Check this works under the alternative.

```{r}
# mean vector
mean_1 <- c(theta_1 * sqrt(n1/ (2*sigma) ),
            theta_1 * sqrt(n2/ (2*sigma) ),
            theta_1 * sqrt(n3/ (2*sigma) ))

# covariance matrix
SIGMA <- matrix(
  c(1, sqrt(n1/n2), sqrt(n1/n3),
    sqrt(n1/n2), 1, sqrt(n2/n3),
    sqrt(n1/n3), sqrt(n2/n3), 1),
  nrow = 3
)

lower <- c(u1, -Inf, -Inf)
upper <- c(Inf, Inf, Inf)

pmvnorm(lower = lower, upper = upper, mean = mean_1, corr = SIGMA)
```

With this information, we can simplify the `gsd_simulations` function that was written last week. We will need a helper function to replace a `for` loop calculating the Fisher information by using `sapply`.

```{r}
# Fisher information
information <- function(n, variance) sqrt(n/(2*variance))
```


```{r}
gsd_simulations <- function(upper_bounds = c(2.5, 2, 1.5),
                            lower_bounds = c(0, 0.75, 1.5),
                            n_patients = c(20, 40, 60),
                            null_hypothesis = 0,
                            alt_hypothesis = 0.5,
                            variance = 1) {
  
  # sanity checks, function stops
  if(length(upper_bounds) != length(lower_bounds)) {
    stop("Warning: number of upper bounds must equal number of lower bounds")
  }
  
  if(length(n_patients) != length(upper_bounds)) {
    stop("Warning: number of patients vector must equal number of analyses")
  }
  
  n_analyses <- length(upper_bounds)
  
  # assign values for null and alt hypotheses
  theta_0 <- null_hypothesis
  theta_1 <- alt_hypothesis
  
  ####################
  # PARSE BOUNDARIES #
  ####################
  
  # need to parse the upper and lower boundaries of the design
  # for futility and efficacy, must put the bounds of integration correctly 
  # for pmvnorm
  futility_l_bounds <- vector(mode = "list", length = n_analyses)
  futility_u_bounds <- vector(mode = "list", length = n_analyses)
  efficacy_l_bounds <- vector(mode = "list", length = n_analyses)
  efficacy_u_bounds <- vector(mode = "list", length = n_analyses)

  for (i in 1:n_analyses) {
      
    # special case of i = 1
    if (i == 1) {
      futility_l_bounds[[i]] <- rep_len(-Inf, length.out = n_analyses)
      futility_u_bounds[[i]] <- c(lower_bounds[i], rep_len(Inf, length.out = n_analyses - 1))
      efficacy_l_bounds[[i]] <- c(upper_bounds[i], rep_len(-Inf, length.out = n_analyses - 1))
      efficacy_u_bounds[[i]] <- rep_len(Inf, length.out = n_analyses)
      next
    }
    
    # all other cases
    futility_l_bounds[[i]] <- c(lower_bounds[1:i-1], rep_len(-Inf, length.out = n_analyses - (i-1)))
    futility_u_bounds[[i]] <- c(upper_bounds[1:i-1], lower_bounds[i], rep_len(Inf, length.out = n_analyses - i ))
  
    efficacy_l_bounds[[i]] <- c(lower_bounds[1:i-1], upper_bounds[i], rep_len(-Inf, length.out = n_analyses - i ))
    efficacy_u_bounds[[i]] <- c(upper_bounds[1:i-1], rep_len(Inf, length.out = n_analyses - (i-1)))
    
  }
  
  ##################
  # GENERATE MEANS #
  ##################
  
  mean_0 <- theta_0 * sqrt(n_patients / (2 * variance))
  mean_1 <- theta_1 * sqrt(n_patients / (2 * variance))
  
  ##############################
  # GENERATE COVARIANCE MATRIX #
  ##############################
  
  # list of SIGMAs
  SIGMA <- diag(nrow = n_analyses)
  
  for(i in 1:n_analyses) {
    for(j  in 1:n_analyses) {
      
      # leave the 1s on the diagonal, skip this iteration of for loop
      if(i == j) next
      
      # when i is less than j, the lower number of patients will be in numerator
      if(i < j) SIGMA[i,j] <- sqrt(n_patients[i] / n_patients[j])
      
      # when i is greater than j, the lower number of patients will be in numerator
      if(i > j) SIGMA[i,j] <- sqrt(n_patients[j] / n_patients[i])
      
    }
  }
  
  #####################
  # GET PROBABILITIES #
  #####################
  
  # list of probabilities to return
  probs_to_return <- matrix(nrow = n_analyses, ncol = 4)
  
  for (i in 1:n_analyses) {
    
    futility_null <- pmvnorm(lower = futility_l_bounds[[i]], 
                             upper = futility_u_bounds[[i]], 
                             mean = mean_0, sigma = SIGMA)
    
    futility_alt <- pmvnorm(lower = futility_l_bounds[[i]],
                            upper = futility_u_bounds[[i]],
                            mean = mean_1, sigma = SIGMA)
    
    efficacy_null <- pmvnorm(lower = efficacy_l_bounds[[i]], 
                             upper = efficacy_u_bounds[[i]], 
                             mean = mean_0, sigma = SIGMA)
    
    efficacy_alt <- pmvnorm(lower = efficacy_l_bounds[[i]],
                            upper = efficacy_u_bounds[[i]],
                            mean = mean_1, sigma = SIGMA)
    
    probs_to_return[i, ] <- c(futility_null, efficacy_null, futility_alt, efficacy_alt)
    
  }
  
  rownames(probs_to_return) <- as.vector(sapply("analysis_", paste0, 1:n_analyses))
  colnames(probs_to_return) <- c("futility_null", "efficacy_null", "futility_alt", "efficacy_alt")
  
  # calculate the expected sample size always under alternative (index 3:4)
  ess <- sum(
    rowSums(probs_to_return[,3:4]) * n_patients
  )
  
  # add the expected sample size to the list
  return_values <- list(probs_to_return, ess)
  
  # name the list
  names(return_values) <- c("probabilities", "expected_sample_size")
  
  # return probabilities and ESS
  return_values
}
```


```{r}
gsd_simulations(alt_hypothesis = 0)
```

Now we can test which is faster. This is the old `gsd_simulations`:

```{r}
gsd_simulations_old <- function(n_analyses = 3, 
                            upper_bounds = c(2.5, 2, 1.5),
                            lower_bounds = c(0, 0.75, 1.5),
                            n_patients = c(20, 40, 60),
                            null_hypothesis = 0,
                            alt_hypothesis = 0.5,
                            variance = 1) {
  
  # sanity checks
  # sanity checks, function stops
  if(length(upper_bounds) != length(lower_bounds)) {
    stop("Warning: number of upper bounds must equal number of lower bounds")
  }
  
  if(length(n_patients) != length(upper_bounds)) {
    stop("Warning: number of patients vector must equal number of analyses")
  }
  
  # assign values for null and alt hypotheses
  theta_0 <- null_hypothesis
  delta <- alt_hypothesis
  
  # empty mean vectors to fill
  mean_0 <- c()
  mean_1 <- c()
  
  # need to parse the upper and lower boundaries of the design
  # for futility and efficacy, must put the bounds of integration correctly 
  # for pmvnorm
  futility_l_bounds <- list()
  futility_u_bounds <- list()
  efficacy_l_bounds <- list()
  efficacy_u_bounds <- list()

  n_analyses <- length(upper_bounds)

  for (i in 1:n_analyses) {
    
    # special case of i = 1
    if (i == 1) {
      futility_l_bounds[[i]] <- lower_bounds[i]
      futility_u_bounds[[i]] <- upper_bounds[i]
      efficacy_l_bounds[[i]] <- lower_bounds[i]
      efficacy_u_bounds[[i]] <- upper_bounds[i]
      next
    }
    
    # all other cases
    futility_l_bounds[[i]] <- c(lower_bounds[1:i-1], -Inf)
    futility_u_bounds[[i]] <- c(upper_bounds[1:i-1], lower_bounds[i])
    
    efficacy_l_bounds[[i]] <- c(lower_bounds[1:i-1], upper_bounds[i])
    efficacy_u_bounds[[i]] <- c(upper_bounds[1:i-1], Inf)
  }
  
  # list of probabilities to return
  probs_to_return <- list()
  
  # list of SIGMAs
  SIGMA_list <- list()
    
  for (i in 1:n_analyses) {
    if (i == 1) next
    
    # start with diagonal matrix for SIGMA
    SIGMA <- diag(nrow = i)
    
    # n = 2, need to fill all but 11, 22
    # n = 3, need to fill all but 11, 22, 33
    # n = 4, need to fill all but 11, 22, 33, 44
    # etc. 
    for(i in 1:i) {
      for(j  in 1:i) {
        
        # leave the 1s on the diagonal, skip this iteration of for loop
        if(i == j) next
        
        # when i is less than j, the lower number of patients will be in numerator
        if(i < j) SIGMA[i,j] <- sqrt(n_patients[i] / n_patients[j])
        
        # when i is greater than j, the lower number of patients will be in numerator
        if(i > j) SIGMA[i,j] <- sqrt(n_patients[j] / n_patients[i])
        
      }
    }
    
    SIGMA_list[[i]] <- SIGMA
  }
  
  
  for (i in 1:n_analyses) {
    
    ##############
    # ANALYSIS 1 #
    ##############
    if(i == 1) {
      # mean under null
      mean_0[i] <- theta_0 * sqrt(n_patients[i]/(2*variance))
      
      # mean under alternative
      mean_1[i] <- delta * sqrt(n_patients[i]/(2*variance))
      
      # prob stop for futility, null
      futility_null <- pnorm(futility_l_bounds[[i]], 
                             mean = mean_0, 
                             sd = sqrt(variance))
      
      # prob stop for efficacy, null
      efficacy_null <- pnorm(efficacy_u_bounds[[i]], 
                             mean = mean_0, 
                             sd = sqrt(variance), 
                             lower.tail = FALSE)
  
      # prob stop for futility, alt
      futility_alt <- pnorm(futility_l_bounds[[i]], 
                            mean = mean_1, 
                            sd = sqrt(variance))
      
      # prob stop for efficacy
      efficacy_alt <- pnorm(efficacy_u_bounds[[i]], 
                            mean = mean_1, 
                            sd = sqrt(variance), 
                            lower.tail = FALSE)
      
      probs_to_return[[i]] <- c(futility_null, efficacy_null, futility_alt, efficacy_alt)
      names(probs_to_return[[i]]) <- c("futility_null", "efficacy_null", "futility_alt", "efficacy_alt")
      
      next
    }
    
    ######################
    # ALL OTHER ANALYSES #
    ######################
    
    # next mean under null
    mean_0[i] <- theta_0 * sqrt(n_patients[i] / (2 * variance))
    
    # next mean under alternative
    mean_1[i] <- delta * sqrt(n_patients[i]/ (2*variance))
    
    # bounds for these will be same
    # futility under null
    futility_null <- pmvnorm(lower = futility_l_bounds[[i]], 
                             upper = futility_u_bounds[[i]], 
                             mean = mean_0, corr = SIGMA_list[[i]])
    # futility under alt
    futility_alt <- pmvnorm(lower = futility_l_bounds[[i]], 
                            upper = futility_u_bounds[[i]], 
                            mean = mean_1, corr = SIGMA_list[[i]])
    
    # bounds for these will be same
    # futility under null
    efficacy_null <- pmvnorm(lower = efficacy_l_bounds[[i]], 
                             upper = efficacy_u_bounds[[i]],
                             mean = mean_0, corr = SIGMA_list[[i]])
    # futility under alt
    efficacy_alt <- pmvnorm(lower = efficacy_l_bounds[[i]], 
                            upper = efficacy_u_bounds[[i]], 
                            mean = mean_1, corr = SIGMA_list[[i]])
    
    probs_to_return[[i]] <- c(futility_null, efficacy_null, futility_alt, efficacy_alt)
    names(probs_to_return[[i]]) <- c("futility_null", "efficacy_null", "futility_alt", "efficacy_alt")
    
  }
    
  # vector to collect the sum of futility and efficacy probabilities
  sum_probs <- c()
  
  # get alpha and power
  alpha <- 0
  power <- 0
  
  for (i in 1:n_analyses) {
    
    # pull the probabilities from the list
    tmp_probs <- probs_to_return[[i]]
    
    # gather them into a vector
    # 3:4 because we want to calculate under the alternative
    sum_probs <- c(sum_probs, sum(tmp_probs[3:4]))
    
    alpha <- tmp_probs[2] + alpha
    power <- tmp_probs[4] + power
    
  }
  
  # calculate the expected sample size
  ess <- sum(n_patients * sum_probs)
  
  # add the expected sample size to the list
  return_values <- append(probs_to_return, values = c(ess, alpha, power))
  
  # name the list
  names_for_list <- as.vector(sapply("analysis_", paste0, 1:n_analyses))
  names_for_list <- c(names_for_list, "expected_sample_size", "alpha", "power")
  names(return_values) <- names_for_list
  
  # return probabilities and ESS
  return_values
}
```


### Which is faster?

```{r}
bench::mark(
  gsd_simulations(),
  gsd_simulations_old(),
  check = FALSE
)
```

The "old" implementation is approximately twice as fast and requires 0.6 times the memory allocation.

Maybe because of the larger `pmvnorm` calls?

```{r}
upper_bounds <- c(0, Inf,  Inf)
lower_bounds <- c(-Inf, -Inf, -Inf)

bench::mark(
  pnorm(0),
  pmvnorm(lower = lower_bounds, upper = upper_bounds,
          mean = mean_0, corr = SIGMA),
  check = FALSE
)
```

```{r}
39.9e-6 / 551.9e-9
```

R's `pnorm` function is approximately 72 times faster than evaluating the multidimensional integral using `pmvnorm`. This likely contributes to the speed discrepancy noted above.

```{r}
# two dimensional sigma
SIGMA2 <- matrix(
  c(1, sqrt(n1/n2), 
    sqrt(n1/n2), 1),
  nrow = 2
)

# two dimensional upper bounds
upper_bounds2 <- c(0, Inf)
lower_bounds2 <- c(-Inf, -Inf)

bench::mark(
  # 2d
  pmvnorm(lower = lower_bounds2, upper = upper_bounds2,
          mean = c(0,0), corr = SIGMA2),
  # 3d
  pmvnorm(lower = lower_bounds, upper = upper_bounds,
          mean = mean_0, corr = SIGMA),
  check = FALSE
)
```

Two and three dimensional integrals appear to be approximately the same speed.

## Maximum ESS with interval bisection

Interval bisection is used to find the optimum of a function with a single global optimum. In this case, the expected sample size as a function of the true value of the difference between two groups ($\delta$) is known to have a single global maximum. 

The iterative process for interval bisection is:
1. Initialize the $\mathbf{x}$ values for the expected sample size function ($\mathbf{x} = \{\delta_1, \delta_2\}$).
2. Calculate the expected sample size for this vector of values.
3. Replace the value of either $\delta_1$ or $\delta_2$ by $\frac{\delta_1 - \delta_2}{2}$ for the $\delta \in \mathbf{x}$ with the lower expected sample size.
4. Repeat until the difference $|\delta_1 - \delta_2 < \epsilon |$, where $\epsilon$ is some small predetermined absolute tolerance.
5. Return $\delta$.

This is the implementation below. It takes, as function values, an expected sample size calculator, an absolute tolerance value $\epsilon$, and the two initial $\delta$s.

```{r}
max_ess <- function(gsd_simulator = gsd_simulations_old, 
                    epsilon = 0.0001,
                    delta1 = 0,
                    delta2 = 2) {
  
  while (abs(delta1 - delta2) > epsilon) {
    
    # calculate expected sample size
    ess1 <- gsd_simulations_old(alt_hypothesis = delta1)$expected_sample_size
    ess2 <- gsd_simulations_old(alt_hypothesis = delta2)$expected_sample_size
    
    # check conditions
    if (ess1 == ess2) {
  
      print("Reinitializing deltas...")
      delta1 <- 0
      delta2 <- delta2 + 0.3
      
    } else if (ess1 > ess2) {
      
      delta2 <- (delta2 + delta1) / 2
      
    } else {
      
      delta1 <- (delta2 + delta1) / 2
      
    }
    
  }
  
  return(
    c(delta1 = delta1, 
      delta2 = delta2, 
      ess1 = ess1, 
      ess2 = ess2)
  )
}
```

```{r}
max_ess()
```


## Triagular designs

a triangular test for one-sided group sequential clinical trial designs.

| Variable          | Meaning                                             |
|-------------------|-----------------------------------------------------|
| $L$               | Number of planned analyses                          |
| $l=\{1,\dots,L\}$ | Analysis $l$ is from 1 to $L$                       |
| $\{0, 1\}$        | Treatment group index                               |
| $n_{0l} = ln$     | Number of patients at $l$th analysis, group 0       |
| $n_{1l} = rln$    | Number of patients at $l$th analysis, group 1       |
| $r$               | Allocation ratio, group 1 relative to group 0       |
| $I_l$             | Fisher information at $l$th analysis                |
| $\delta$          | Clinically relevant difference                      |
| $\alpha$          | Type I error                                        |
| $\beta$           | Type II error                                       |
| $e_l$             | Upper stopping boundary for $l$th analysis          |
| $f_l$             | Lower stopping boundary for $l$th analysis          |
| $z_p$             | Value of standard normal cdf up to probability $p$  |

Recall that, for this case of a normally distributed outcome variable, the Fisher information is
$$
I_l = \left( \frac{\sigma_0^2}{n_{0l}} + \frac{\sigma_1^2}{n_{1l}} \right)^{-1} = \left( \frac{\sigma_0^2}{ln} + \frac{\sigma_1^2}{rln} \right)^{-1}
$$

Assuming that $\sigma_0$ and $\sigma_1$ are equal, $\sigma = \sigma_0 = \sigma_1$, and that $r = 1$, then $I_l$ simplifies to
$$
I_l = \frac{ln}{2\sigma^2}
$$

Using the above, we write the bounds for the triangular test:

$$
e_l = \left[ \frac{2}{\widetilde{\delta}} \log\left( \frac{1}{2\alpha} \right) - 0.583\left( \frac{I_L}{L} \right)^{0.5} + \frac{\widetilde{\delta}}{4} \cdot \frac{l}{L} \cdot I_L \right]\frac{1}{I_l^{0.5}}
$$

$$
f_l = \left[ -\frac{2}{\widetilde{\delta}} \log\left( \frac{1}{2\alpha} \right) + 0.583\left( \frac{I_L}{L} \right)^{0.5} + \frac{3\widetilde{\delta}}{4} \cdot \frac{l}{L} \cdot I_L \right]\frac{1}{I_l^{0.5}}
$$

where
$$
\widetilde{\delta} = \frac{2\,z_{1-\alpha/2}\, \delta}{z_{1-\alpha/2}+z_{1-\beta}}
$$

and
$$
I_L = \left[ \left\{ \frac{4(0.583)^2}{L} + 8\log\left(\frac{1}{2\alpha}\right)  \right\}^{0.5} -\frac{2(0.583)}{L^{0.5}} \right]^2 \frac{1}{\widetilde{\delta}}
$$

Write the code to calculate the boundaries.

```{r}
triangular_bounds <- function(n_analyses = 3,
                              alpha = 0.05,
                              beta = 0.1,
                              delta = 0.5,
                              allocation_ratio = 1,
                              n = 20,
                              sigma_0 = 1,
                              sigma_1 = 1) {
  
  # I think this needs to be alpha rather that alpha/2 as this is one-sided
  delta_tild <- (2 * qnorm( 1-alpha ) * delta) / (qnorm( 1-alpha ) + qnorm( 1-beta ))
  
  log_1over2alpha <- log(1/(2*alpha))
  
  I_L_term1 <- (4 * 0.583^2) / n_analyses
  I_L_term2 <- 8 * log_1over2alpha
  I_L_term3 <- (2 * 0.583) / sqrt(n_analyses)
  
  I_L <- ( sqrt(I_L_term1 + I_L_term2) - I_L_term3 )^2 * (1 / delta_tild)
  
  e <- vector(mode = "numeric", length = n_analyses)
  f <- vector(mode = "numeric", length = n_analyses)
  
  bounds_term1 <- (2/delta_tild) * log_1over2alpha
  bounds_term2 <- 0.583 * sqrt(I_L / n_analyses)
  
  for (i in 1:n_analyses) {
    
    information_term1 <- sigma_0 / (i * n)
    information_term2 <- sigma_1 / (allocation_ratio * i * n)
    
    information <- 1 / (information_term1 + information_term2)
    
    sqrt_inv_info_l <- 1 / sqrt(information)
    
    e_l <- (bounds_term1 - bounds_term2 + ( (0.25*delta_tild) * (i/n_analyses) * I_L ) ) * sqrt_inv_info_l
  
    f_l <- ( (-1*bounds_term1) + bounds_term2 + ( (0.75*delta_tild) * (i/n_analyses) * I_L ) ) * sqrt_inv_info_l
  
    e[i] <- e_l
    f[i] <- f_l
    
  }
  
  return(list(e = e, f = f))
}
```


Confirm that these calculated boundaries are correct using the `gsd_simulation_old`:

```{r}
delta <- 0.5

bounds <- triangular_bounds(delta = delta, beta = 0.1, alpha = 0.05)

upper <- bounds$e
lower <- bounds$f 

gsd_simulations_old(n_analyses = 3,
                    upper_bounds = upper,
                    lower_bounds = lower,
                    n_patients = c(20, 40, 60),
                    null_hypothesis = 0,
                    alt_hypothesis = delta,
                    variance = 1)
```



```{r}
ggplot() +
  geom_line(mapping = aes(x = c(1,2,3), y = upper)) +
  geom_line(mapping = aes(x = c(1,2,3), y = lower))
```





















