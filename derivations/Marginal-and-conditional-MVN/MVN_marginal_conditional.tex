\documentclass{article}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{xfrac}
\usepackage{ulem}
\usepackage{mathtools}

\usepackage[sorting=none]{biblatex} % Imports biblatex package
\addbibresource{references.bib} % Import the bibliography file

\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\title{Derivation of Multivariate Normal marginal and conditional distributions}
\author{Mazin Abdelghany, MD, MS}
\date{3 December 2025}

\begin{document}

\maketitle

\section{Background}

Gaussian process regression relies on several special properties of multivariate normal (MVN) distributions. The two most important of those properties are that:
\begin{enumerate}
\item The conditional distribution of a multivariate normal is normal
\item The marginal distribution of a multivariate normal is normal
\end{enumerate}

Moreover, these two above items can be calculated in closed form. Suppose that we have an $n$-dimensional Gaussian\footnote{Gaussian and normal will be interchanged throughout this document and used as synonyms.} random vector\footnote{Throughout this document, vectors will be denoted using lowercased bold letters while matrices will be denoted as uppercase bold letters.} $\bm{x}\in\mathbb{R}^n$. That is, 
\[
\bm{x} = \begin{bmatrix}
x_1 \sim \mathcal{N}(\mu_1, \sigma_1^2) \\
x_2 \sim \mathcal{N}(\mu_2, \sigma_2^2) \\
\vdots\\
x_n \sim \mathcal{N}(\mu_3, \sigma_3^2) 
\end{bmatrix}
\]

Using vector-matrix notation, we can rewrite the distribution statement as:
\[
\bm{x} \sim \mathcal{N}_n(\bm{\mu}, \bm{\Sigma})
\]

with the subscipt $n$ indicating that vector $\bm{x}$ is $n$-dimensional.

In this case, $\bm{\mu}$ has become a mean \textit{\textbf{vector}} of (possibly different) means for each of our random variables and $\bm{\Sigma}$ has become a covariace \textit{\textbf{matrix}}:
\begin{align*}
\bm{\mu} = \begin{bmatrix}
\mu_1\\
\mu_2\\
\vdots\\
\mu_n
\end{bmatrix} &&
\bm{\Sigma} = \begin{bmatrix}
\sigma_{11} & \sigma_{12} & \cdots & \sigma_{1n} \\
\sigma_{21} & \sigma_{22} & \cdots & \sigma_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{n1} & \sigma_{n2} & \cdots & \sigma_{nn} 
\end{bmatrix}
\end{align*}

We can make a few observations about this formulation:
\begin{enumerate}
\item $\bm{\mu} \in \mathbb{R}^n$ and $\bm{\Sigma}\in\mathbb{R}^{n\times n}$ (i.e., the mean is $n$-dimensional and the covariance matrix is $n\times n$-dimensional).
\item $\bm{\mu}$ is the same number of dimensions as $\bm{x}$.
\item $\bm{\mu}$ can assign a different $\mu_i$ for each $x_i \in \bm{x}$.
\item $\bm{\Sigma}$ is symmetric and all its entries are positive or zero (more strictly, it must be positive semidefinite).
\item The diagnoal of $\bm{\Sigma}$ is the covariance between $x_i$ and itself, which is the same as the variance of $x_i$ (i.e., $\sigma_{11} = \sigma_1^2$).
\end{enumerate}

The pdf of a multivariate Gaussian is:
\[
f(\bm{x}) = \frac{1}{ (2\pi)^{\sfrac{n}{2}} \det(\bm{\Sigma})^{\sfrac{1}{2}}  } \exp \left\{-\frac{1}{2} (\bm{x} - \bm{\mu})^\top \bm{\Sigma}^{-1} (\bm{x} - \bm{\mu})  \right\}
\]

Now, let us partition our multivariate normal random vector $\bm{x}\in\mathbb{R}^n$ into two multvariate normal random vectors $\bm{x}_a\in\mathbb{R}^d$ and $\bm{x}_b\in\mathbb{R}^{(n-d)}$:\footnote{Note the dimensions of each vector!}
\[
\bm{x} = \begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix}
\]

This new partitioned vector should have a multivariate normal distribution denoted by
\begin{equation}\label{a}
\begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix} = \mathcal{N}\left( \begin{bmatrix}
\bm{u}_a \\
\bm{u}_b
\end{bmatrix}, \begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix} \right)
\end{equation}

With this background in mind, we can proceed to show that

\begin{enumerate}
\item The conditional mean vector and covariance matrix of a multivariate normal for the vector $\bm{x}_a$ given a realization of the vector $\bm{x}_b$ are
\begin{itemize}
\item $\bm{\mu}_{\bm{x}_a\,|\,\bm{x}_b} = \bm{\mu_a} + \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b-\bm{\mu}_b)$
\item $\bm{\Sigma}_{\bm{x}_a\,|\,\bm{x}_b} = \bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba}$
\end{itemize}
\item The marginal mean vector and covariance matrix of a multivariate normal for $\bm{x}_a$ are
\begin{itemize}
\item $\bm{\mu}_{\bm{x}_a} = \bm{\mu}_a$
\item $\bm{\Sigma}_{\bm{x}_a} = \bm{\Sigma}_{aa}$
\end{itemize}
\end{enumerate}

\section{Marginal of mulitvariate normal}

Let us start with proof of the marginal mean vector and covariance matrix. The marginal pdf of $\bm{x}_a$ is:
\begin{align*}
f(\bm{x}_a) = \int_{\mathbb{R}^{(n-d)}} f(\bm{x}_a, \bm{x}_b)\, d\bm{x}_b
\end{align*}

because we are integrating out $\bm{x}_b$ leaving only the pdf of $\bm{x}_a$. Plugging in the values of the mean vectors and covariance matrices from equation \eqref{a}, we can write
\begin{align*}
f(\bm{x}_a) &= \int_{\mathbb{R}^{(n-d)}} \frac{1}{ (2\pi)^{\sfrac{n}{2}} \det(\bm{\Sigma})^{\sfrac{1}{2}}  } \exp \left\{-\frac{1}{2} \left(
\begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix} - 
\begin{bmatrix}
\bm{u}_a \\
\bm{u}_b
\end{bmatrix}\right)^\top 
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}^{-1} 
\left(\begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix} - 
\begin{bmatrix}
\bm{u}_a \\
\bm{u}_b
\end{bmatrix}\right)  \right\}\, d\bm{x}_b \\
&= \int_{\mathbb{R}^{(n-d)}} \frac{1}{ (2\pi)^{\sfrac{n}{2}} \det(\bm{\Sigma})^{\sfrac{1}{2}}  } \exp \left\{-\frac{1}{2} \left(
\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}\right)^\top 
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}^{-1} 
\left(\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}\right)  \right\}\, d\bm{x}_b
\end{align*}

First, to simplify notation, let us define the normalizing constant as
\[
Q = (2\pi)^{\sfrac{n}{2}} \det(\bm{\Sigma})^{\sfrac{1}{2}}  
\]

Now, we need to introduce the concept of the precision matrix $\bm{\Lambda}$. The precision matrix is defined as the inverse of the covariance matrix: $\bm{\Lambda}=\bm{\Sigma}^{-1}$. Using this new definition, we note that
\[
\bm{\Lambda}=
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}=
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}^{-1}=
\bm{\Sigma}^{-1}
\]

Now we can rewrite the above as:
\begin{equation}
\label{eq2}
f(\bm{x}_a) = \int_{\mathbb{R}^{(n-d)}} \frac{1}{ Q } \exp \left\{-\frac{1}{2} 
\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}^\top 
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}
\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}  \right\}\, d\bm{x}_b
\end{equation}

It is important to note that $\bm{\Lambda}_{aa} \neq \bm{\Sigma}_{aa}^{-1}$ but rather that $\bm{\Lambda}=\bm{\Sigma}^{-1}$!

Let us expand the positive of the expression within the exponent $\exp\{\cdot\}$ in two steps:
\[
\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}^\top 
\begin{bmatrix}
\bm{\Lambda}_{aa} (\bm{x}_a - \bm{u}_a) + \bm{\Lambda}_{ab} (\bm{x}_b - \bm{u}_b) \\
\bm{\Lambda}_{ba} (\bm{x}_a - \bm{u}_a) + \bm{\Lambda}_{ab} (\bm{x}_b - \bm{u}_b)
\end{bmatrix}
\]
\begin{multline}
\label{eq3}
\frac{1}{2}(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{aa}(\bm{x}_a - \bm{u}_a) + \frac{1}{2}(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{ab}(\bm{x}_b - \bm{u}_b) + \\
+ \frac{1}{2}(\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{ba}(\bm{x}_a - \bm{u}_a) + \frac{1}{2}(\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{bb}(\bm{x}_b - \bm{u}_b) 
\end{multline}

At this point we need to take advantage of the concept of completing the square. 
\begin{theorem}
\label{thrm1}
Let $\bm{A}\in\mathbb{R}^{n \times n}$ be a symmetric, positive definite matrix and $\{\bm{z}, \bm{b}, \bm{c}\} \in \mathbb{R}^n$, then
\[
\frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\bm{z} + \bm{c} = \frac{1}{2}(\bm{z} + \bm{A}^{-1}\bm{b})^\top\bm{A}(\bm{z}+\bm{A}^{-1}\bm{b})+\bm{c} - \frac{1}{2}\bm{b}^\top\bm{A}^{-1}\bm{b}
\] 
\end{theorem}
\begin{lemma}
Note that $\bm{b}^\top \bm{z}$ is a scalar ($\mathbb{R}^{1\times n} \cdot \mathbb{R}^{n \times 1} = 1 \times 1$). Note also that 
\begin{equation*}
\begin{aligned}
\bm{b}^\top \bm{z} &= \begin{bmatrix}
b_1 & b_2 & \dots & b_n
\end{bmatrix}\begin{bmatrix}
z_1 \\ z_2 \\ \dots \\ z_n
\end{bmatrix}\\
&=b_1z_1+b_2z_1+\dots+b_nz_n
\end{aligned}\qquad
\begin{aligned}
\bm{z}^\top \bm{b} &= \begin{bmatrix}
z_1 & z_2 & \dots & z_n
\end{bmatrix}\begin{bmatrix}
b_1 \\ b_2 \\ \dots \\ b_n
\end{bmatrix}\\
&=z_1b_1+z_1b_2+\dots+z_nb_n
\end{aligned}
\end{equation*}
By the commutative property of multiplication of scalars, $\bm{b}^\top \bm{z} = \bm{z}^\top \bm{b}$.
\end{lemma}
\begin{proof}
Thus,
\[
\frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\bm{z} + \bm{c} = \frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \underbrace{\frac{1}{2}\bm{b}^\top\bm{z} + \frac{1}{2}\bm{z}^\top\bm{b}}_{\text{from lemma}} + \bm{c}
\]
Then, add and subtract $\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}$
\begin{align*}
&= \frac{1}{2} \bm{z}^\top \bm{A}\bm{z} +\frac{1}{2}\bm{b}^\top\bm{z} + \frac{1}{2}\bm{z}^\top\bm{b} + \bm{c} + \left(\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\right)\\
&\text{Rearrange terms.}\\
&= \frac{1}{2} \bm{b}^\top\bm{z} + \frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b} + \frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \frac{1}{2}\bm{z}^\top\bm{b} + \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Pull out the $\frac{1}{2}$.}\\
&= \frac{1}{2} (\bm{b}^\top\bm{z} + \bm{b}^\top \bm{A}^{-1}\bm{b} + \bm{z}^\top \bm{A}\bm{z} +\bm{z}^\top\bm{b} )+ \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Rearrange terms.}\\
&= \frac{1}{2} (\bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\bm{z}  +\bm{z}^\top\bm{b} + \bm{b}^\top \bm{A}^{-1}\bm{b})+ \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
\end{align*}
We can now make a comparison to the scalar version of completing the square:
\[
=\frac{1}{2} (\underbrace{\bm{z}^\top \bm{A}\bm{z}}_{\text{like $Ax^2$}} + 
\underbrace{\bm{b}^\top\bm{z}  +\bm{z}^\top\bm{b}}_{\text{like $2zb$}} + 
\underbrace{\bm{b}^\top \bm{A}^{-1}\bm{b}}_{\text{like $\frac{1}{A}b^2$}})
+ \bm{c} - \frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}
\]
In order to factorize the above in parentheses, we need to fill in some blanks:
\[
\frac{1}{2} (\bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\text{\underline{\phantom{xx}} }\bm{z}  +\bm{z}^\top\text{\underline{\phantom{xx}} }\bm{b} + \bm{b}^\top \bm{A}^{-1}\text{\underline{\phantom{xx}} }\bm{b})+ \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}
\]
To leave the equation the same, we multiply by the identity matrix: $\bm{I} = \bm{A}\bm{A}^{-1}$.
\begin{align*}
&=\frac{1}{2} (\bm{z}^\top \bm{A}\bm{z} +\bm{z}^\top\bm{A}\bm{A}^{-1}\bm{b} + \bm{b}^\top\bm{A}^{-1}\bm{A}\bm{z}   + \bm{b}^\top \bm{A}^{-1}\bm{A}\bm{A}^{-1}\bm{b})+ \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Pull out $(\bm{z} + \bm{A}^{-1}\bm{b})$ on the right.}\\
&=\frac{1}{2} ( \bm{z}^\top \bm{A} + \bm{b}^\top\bm{A}^{-1}\bm{A})(\bm{z} + \bm{A}^{-1}\bm{b} ) + \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Pull out $\bm{A}$ from the first expression.}\\
&=\frac{1}{2} ( \bm{z}^\top + \bm{b}^\top\bm{A}^{-1})\bm{A}(\bm{z} + \bm{A}^{-1}\bm{b} ) + \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Remove a transpose from the first expression.}\\
\frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\bm{z} + \bm{c}&=\frac{1}{2} ( \bm{z} + \bm{A}^{-1}\bm{b})^\top\bm{A}(\bm{z} + \bm{A}^{-1}\bm{b} ) + \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
\end{align*}
\end{proof}

Back to the problem at hand. Let us complete the square from equation \eqref{eq3}:
\begin{multline*}
\frac{1}{2}(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{aa}(\bm{x}_a - \bm{u}_a) + \frac{1}{2}(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{ab}(\bm{x}_b - \bm{u}_b) + \\
+ \frac{1}{2}(\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{ba}(\bm{x}_a - \bm{u}_a) + \frac{1}{2}(\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{bb}(\bm{x}_b - \bm{u}_b) 
\end{multline*}

Define the following recalling that all precision and covariance matrices are positive semidefinite:
\begin{equation*}
\begin{aligned}
\bm{A} = \bm{\Lambda}_{bb}
\end{aligned} \qquad
\begin{aligned}
\bm{z} &= \bm{x}_b - \bm{\mu}_b\\
\bm{b} &= \bm{\Lambda}_{ba} (\bm{x}_b - \bm{\mu}_b)\\
\bm{c} &= \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa} (\bm{x}_a - \bm{\mu}_a)
\end{aligned}
\end{equation*}

Note also that, using the fact that $\bm{\Lambda}_{ba}^\top = \bm{\Lambda}_{ab}$,
\[
\bm{b} = \bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \implies \bm{b}^\top =  (\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}
\]

We note,
\begin{align*}
\frac{1}{2} \bm{z}^\top \bm{A}\bm{z} &= \frac{1}{2} (\bm{x}_b - \bm{\mu}_b)^\top\bm{\Lambda}_{bb}(\bm{x}_b - \bm{\mu}_b) \\
\bm{b}^\top\bm{z} &= (\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}(\bm{x}_b - \bm{\mu}_b)\\
\bm{c} &=  \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa} (\bm{x}_a - \bm{\mu}_a)
\end{align*}

And applying Theorem \ref{thrm1},
\begin{multline}
\label{eq4}
\frac{1}{2} \Big[ \big(  (\bm{x}_b - \bm{\mu}_b) +\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)^\top \bm{\Lambda}_{bb}  \big(  (\bm{x}_b - \bm{\mu}_b) + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)\Big] \\
+ \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) - \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a)
\end{multline}

Keeping track of the $-1$ that we left out when completing the square, and combining \eqref{eq4} with \eqref{eq2}, the full expression becomes:
\begin{multline*}
f(\bm{x}_a) = \frac{1}{Q} \int_{\mathbb{R}^{n-d}} \exp \left\{ -\frac{1}{2} \Big[ \big(  (\bm{x}_b - \bm{\mu}_b) +\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)^\top \bm{\Lambda}_{bb}  \big(  (\bm{x}_b - \bm{\mu}_b) +\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)\Big] \right. \\
\left. - \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) + \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a) \right\}\, d\bm{x}_b
\end{multline*}

Since we are integrating over $\bm{x}_b$, all of the terms that contain $\bm{x}_a$ can be taken outside of the integral. Using $e^{ab} = e^ae^b$,
\begin{multline}
\label{eq5}
f(\bm{x}_a) = \frac{1}{Q} \exp \left\{-\frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) + \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a) \right\}\\ \int_{\mathbb{R}^{n-d}} \exp \left\{ -\frac{1}{2} \Big[ \big(  (\bm{x}_b - \bm{\mu}_b) + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)^\top \bm{\Lambda}_{bb}  \big(  (\bm{x}_b - \bm{\mu}_b) +\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)\Big]
   \right\}\, d\bm{x}_b
\end{multline}

We can remove the inner parentheses $ (\bm{x}_b - \bm{\mu}_b) + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) $ and instead write $\bm{x}_b - \bm{\mu}_b + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a)$.

We now observe that if we replace $ \bm{\mu}_b + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) $ with $\bm{m}$, the above integral in equation \eqref{eq5} becomes
\[
\int_{\mathbb{R}^{n-d}} \exp \left\{ -\frac{1}{2} \Big[ \big(  (\bm{x}_b - \bm{m} \big)^\top \bm{\Lambda}_{bb}  \big(  (\bm{x}_b - \bm{m} \big)\Big]
   \right\}\, d\bm{x}_b
\]

Note, that this integral is a multivariate normal kernel with mean vector $\bm{m}$ and covariance matrix $\bm{\Lambda}_{bb}$ and that we are integrating over the correct dimensionality of $\bm{x}_b$! Therefore, the integral evaluates to the normalizing constant
\[
(2\pi)^{\frac{n-d}{2}} \det(\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}}
\]

Plugging this into equation \eqref{eq5}, we write
\begin{multline*}
f(\bm{x}_a) = \frac{1}{Q} (2\pi)^{\frac{n-d}{2}} \det(\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}}\\  \exp \left\{\frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) + \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a) \right\}
\end{multline*}

Pulling out a $-\frac{1}{2}$, 
\begin{multline*}
f(\bm{x}_a) = \frac{1}{Q} (2\pi)^{\frac{n-d}{2}} \det(\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}}\\  \exp \left\{-\frac{1}{2}\left[ (\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) - (\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a)\right]   \right\}
\end{multline*}

Then pulling out a $(\bm{x}_a - \bm{\mu}_a)^\top$ from the left and an $(\bm{x}_a - \bm{\mu}_a)$ from the right, we get
\begin{multline}
\label{eq6}
f(\bm{x}_a) = \frac{1}{Q} (2\pi)^{\sfrac{n-d}{2}} \det(\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}}\\  \exp \left\{-\frac{1}{2}\left[ (\bm{x}_a - \bm{\mu}_a)^\top \bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a)\right]   \right\}
\end{multline}

Equation \eqref{eq6} appears to be a MVN with mean vector $\bm{\mu}_a$ and covariance matrix $\bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}$. Now, all that is left is to rewrite the covariance matrix and the normalization constant in terms of $\bm{\Sigma}$.

Recall,
\[
\bm{\Lambda}=
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}=
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}^{-1}=
\bm{\Sigma}^{-1}
\]

Thus, $\bm{\Lambda}^{-1} = \bm{\Sigma}$ and
\[
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}^{-1}=
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}
\]

We now need a result from linear algebra.

\begin{theorem}
\label{thrm3}
Let $\bm{M}$ be a symmetric, positive definite block matrix,
\[
\bm{M} = \begin{bmatrix}
\bm{A} & \bm{B} \\
\bm{C} & \bm{D}
\end{bmatrix}
\]
We can define $\bm{M}^{-1}$ using the Shur complement, $\bm{D}$ in $\bm{M}$: $(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1}$.
\[
\begin{bmatrix}
\bm{A} & \bm{B} \\
\bm{C} & \bm{D}
\end{bmatrix}^{-1} = \begin{bmatrix}
(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1} & -(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1}\bm{B}\bm{D}^{-1} \\
-\bm{D}\bm{C}(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1} & \bm{D}^{-1} + \bm{D}^{-1}\bm{C}(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1}\bm{B}\bm{D}^{-1}
\end{bmatrix}
\]
\end{theorem}

Using Theorem \ref{thrm3}, we write
\begin{multline}
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}=
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}^{-1}\\=
\begin{bmatrix}
(\bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} ) ^{-1} & -(\bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} ) ^{-1}\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1} \\
-\bm{\Lambda}_{bb}\bm{\Lambda}_{ba} (\bm{\Lambda}_{aa}- \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} ) ^{-1} & \bm{\Lambda}_{bb}^{-1} + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} ) ^{-1}\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}
\end{bmatrix}
\end{multline}

Reading across, we see that
\begin{align*}
\bm{\Sigma}_{aa} &=(\bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} ) ^{-1} \\
\bm{\Sigma}_{aa}^{-1} &=\bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} 
\end{align*}

Lastly, we handle the normalization constant. We need one more result from linear algebra regarding the determinant of block matrices:
\begin{theorem}
\label{thrm4}
Again, let $\bm{M}$ be a symmetric, positive definite block matrix,
\[
\bm{M} = \begin{bmatrix}
\bm{A} & \bm{B} \\
\bm{C} & \bm{D}
\end{bmatrix}
\]
The determinant of $\bm{M}$ is
\[
\det(\bm{M}) = \det(\bm{A})\det(\bm{D} - \bm{C}\bm{A}^{-1}\bm{B})
\]
\end{theorem}

From equation \eqref{eq6}, we pull the constant, and replace $Q$ with its value,
\begin{align}
\label{eq8}
\frac{1}{Q} (2\pi)^{\frac{n-d}{2}} \det((\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}}) &= (2\pi)^{\frac{-n}{2}} \det(\bm{\Sigma})^{-\sfrac{1}{2}} (2\pi)^{\frac{n-d}{2}} \det((\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}})
\end{align}

Using theorem \ref{thrm4}, the determinant of the block matrix $\bm{\Sigma}$ is
\[\det(\bm{\Sigma}) = \det(\bm{\Sigma}_{aa})\det(\Sigma_{bb} - \bm{\Sigma}_{ba}\bm{\Sigma}_{aa}^{-1}\bm{\Sigma}_{ab})\]

Plugging this into equation \eqref{eq8} and distributing the exponent $-\sfrac{1}{2}$, we can write,
\begin{align*}
&=(2\pi)^{\frac{-n}{2}} \det(\bm{\Sigma}_{aa})^{-\sfrac{1}{2}}\det(\bm{\Sigma}_{bb} - \bm{\Sigma}_{ba}\bm{\Sigma}_{aa}^{-1}\bm{\Sigma}_{ab})^{-\sfrac{1}{2}} (2\pi)^{\frac{n-d}{2}} \det((\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}})\\
&\text{Rearranging terms.}\\
&=(2\pi)^{\frac{-n}{2}}(2\pi)^{\frac{n-d}{2}}\det(\bm{\Sigma}_{aa})^{-\sfrac{1}{2}}\det(\bm{\Sigma}_{bb} - \bm{\Sigma}_{ba}\bm{\Sigma}_{aa}^{-1}\bm{\Sigma}_{ab})^{-\sfrac{1}{2}}\det((\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}})\\
&\text{Combining exponents over $\pi$.}\\
&=(2\pi)^{\frac{-d}{2}}\det(\bm{\Sigma}_{aa})^{-\sfrac{1}{2}}\det(\bm{\Sigma}_{bb} - \bm{\Sigma}_{ba}\bm{\Sigma}_{aa}^{-1}\bm{\Sigma}_{ab})^{-\sfrac{1}{2}}\det((\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}})
\end{align*}

And if
\[\bm{\Sigma}_{aa} =(\bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} ) ^{-1} \]
then,
\[\bm{\Sigma}_{bb} =(\bm{\Lambda}_{bb} - \bm{\Lambda}_{ba}\bm{\Lambda}_{aa}^{-1}\bm{\Lambda}_{ab} ) ^{-1} \]
and likewise
\begin{align*}
\bm{\Lambda}_{bb} &= (\bm{\Sigma}_{bb}- \bm{\Sigma}_{ba} \bm{\Sigma}_{aa}^{-1} \bm{\Sigma}_{ab})^{-1}\\
\bm{\Lambda}_{bb}^{-1} &= (\bm{\Sigma}_{bb}- \bm{\Sigma}_{ba} \bm{\Sigma}_{aa}^{-1} \bm{\Sigma}_{ab})
\end{align*}

Replacing the $\bm{\Lambda}_{bb}^{-1}$, we write
\[
(2\pi)^{\frac{-d}{2}}\det(\bm{\Sigma}_{aa})^{-\sfrac{1}{2}}\det(\bm{\Sigma}_{bb} -\bm{\Sigma}_{ba}\bm{\Sigma}_{aa}^{-1}\bm{\Sigma}_{ab})^{-\sfrac{1}{2}}\det(\bm{\Sigma}_{bb}- \bm{\Sigma}_{ba} \bm{\Sigma}_{aa}^{-1} \bm{\Sigma}_{ab})^{\sfrac{1}{2}}
\]

The two determinants on the right cancel leaving the final normalizing constant as
\[(2\pi)^{\frac{-d}{2}}\det(\bm{\Sigma}_{aa})^{-\sfrac{1}{2}}\]

We can now rewrite equation \eqref{eq6} as
\begin{align}
f(\bm{x}_a) &= \frac{1}{(2\pi)^{\frac{-d}{2}}\det(\bm{\Sigma}_{aa})^{-\sfrac{1}{2}}}  \exp \left\{-\frac{1}{2}\left[ (\bm{x}_a - \bm{\mu}_a)^\top \underbrace{\bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}}_{\bm{\Sigma}_{aa}^{-1}}(\bm{x}_a - \bm{\mu}_a)\right]   \right\}\\
\label{eq10}
&=\frac{1}{(2\pi)^{\frac{d}{2}}\det(\bm{\Sigma}_{aa})^{\sfrac{1}{2}}}  \exp \left\{-\frac{1}{2}\left[ (\bm{x}_a - \bm{\mu}_a)^\top \bm{\Sigma}_{aa}^{-1}(\bm{x}_a - \bm{\mu}_a)\right]   \right\}
\end{align}

Recall that we defined $\bm{x}_a$ as a multivariate normal random vector in $\mathbb{R}^d$. Therefore, miraculously, the exponent in the normalizing constant is correct! Furthermore, it is clear that the last equation \eqref{eq10}, is the multivariate normal pdf with mean vector $\bm{\mu}_a$ and covariance matrix $\bm{\Sigma}_{aa}$. Thus,
\[p(\bm{x}_{a}) \sim \mathcal{N}(\bm{\mu}_{a}, \bm{\Sigma}_{aa})\qquad\]
and likewise
\[p(\bm{x}_{b}) \sim \mathcal{N}(\bm{\mu}_{b}, \bm{\Sigma}_{bb})\qquad\square\]

\section{Conditional of multivariate normal}

Remaining in the same setup, our multivariate normal random vector $\bm{x}\in\mathbb{R}^n$ has two multvariate normal random vector partitions $\bm{x}_a\in\mathbb{R}^d$ and $\bm{x}_b\in\mathbb{R}^{(n-d)}$.
\[
\bm{x} = \begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix}
\]

This new partitioned vector should have a multivariate normal distribution denoted by
\begin{equation}\label{a}
\begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix} = \mathcal{N}\left( \begin{bmatrix}
\bm{u}_a \\
\bm{u}_b
\end{bmatrix}, \begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix} \right)
\end{equation}

We now are looking to solve for $p(\bm{x}_a \,|\, \bm{x}_b)$. Using Bayes' theorem, we have
\[p(\bm{x}_a \,|\, \bm{x}_b) = \frac{p(\bm{x}_a, \bm{x}_b)}{p(\bm{x}_b)} \propto p(\bm{x}_a, \bm{x}_b) \]

Using the definition of the pdf of a multivariate normal and the proof of the marginal distribution of a multivariate normal that was just proven, we write,
\begin{align*}
p(\bm{x}_a \,|\, \bm{x}_b) &= \frac{
\frac{1}{(2\pi)^{\frac{n}{2}}\det(\bm{\Sigma})^{\sfrac{1}{2}}}  \exp \left\{-\frac{1}{2}\left[ (\bm{x} - \bm{\mu})^\top \bm{\Sigma}^{-1}(\bm{x}- \bm{\mu})\right]   \right\}
}{
\frac{1}{(2\pi)^{\frac{n-d}{2}}\det(\bm{\Sigma}_{bb})^{\sfrac{1}{2}}}  \exp \left\{-\frac{1}{2}\left[ (\bm{x}_b - \bm{\mu}_b)^\top \bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b)\right]   \right\}
}
\end{align*}

Clean up the fraction and combine the exponents.
\begin{multline*}
=\frac{(2\pi)^{\frac{n-d}{2}}}{(2\pi)^{\frac{n}{2}}}\frac{\det(\bm{\Sigma}_{bb})^{\sfrac{1}{2}}}{\det(\bm{\Sigma})^{\sfrac{1}{2}}}
\exp\left\{
-\frac{1}{2}\left[ (\bm{x} - \bm{\mu})^\top \bm{\Sigma}^{-1}(\bm{x}- \bm{\mu})\right] \right. \\ \left. +\frac{1}{2}\left[ (\bm{x}_b - \bm{\mu}_b)^\top \bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b)\right]
\right\}
\end{multline*}

We can use the same method of partitioning as equation \eqref{eq2} to write
\begin{multline*}
=\frac{1}{(2\pi)^{\frac{d}{2}}}\frac{\det(\bm{\Sigma}_{bb})^{\sfrac{1}{2}}}{\det(\bm{\Sigma})^{\sfrac{1}{2}}}
\exp\left\{
-\frac{1}{2} \begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}^\top 
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}
\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix} \right. \\ \left. +\frac{1}{2}\left[ (\bm{x}_b - \bm{\mu}_b)^\top \bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b)\right]
\right\}
\end{multline*}

Removing the normalization constant for ease, and expanding the matrix multiplications as we did above in equation \eqref{eq3},
\begin{multline}
\label{eq12}
\exp
\left\{
-\frac{1}{2} 
\left(
(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{aa}(\bm{x}_a - \bm{u}_a) + (\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{ab}(\bm{x}_b - \bm{u}_b) 
\right.\right.\\
+ (\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{ba}(\bm{x}_a - \bm{u}_a) + (\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{bb}(\bm{x}_b - \bm{u}_b) 
\\\left.\left.
+\frac{1}{2}(\bm{x}_b - \bm{\mu}_b)^\top \bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b)
\right)
\right\}
\end{multline}

Note that 
\[
\underbrace{(\bm{x}_b - \bm{u}_b) ^\top}_{1 \times (n-d)}
\underbrace{\bm{\Lambda}_{ba}(\bm{x}_a }_{(n-d) \times d}
- \underbrace{\bm{u}_a)}_{d \times 1}
\]
is a scalar quantity. Therefore, we can transpose it without concern that it will change the value of the above expression \eqref{eq12}. Thus, we can write,
\begin{align*}
\left[(\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{ba}(\bm{x}_a - \bm{u}_a)\right]^\top &= (\bm{x}_a - \bm{u}_a)^\top \bm{\Lambda}_{ba}^\top(\bm{x}_b - \bm{u}_b) \\
&\text{Using } \bm{\Lambda}_{ba}^\top = \bm{\Lambda}_{ab} \text{ by symmetry}\\
&=(\bm{x}_a - \bm{u}_a)^\top \bm{\Lambda}_{ab}(\bm{x}_b - \bm{u}_b)
\end{align*}

There are now $2(\bm{x}_a - \bm{u}_a)^\top \bm{\Lambda}_{ab}(\bm{x}_b - \bm{u}_b)$ in expression \eqref{eq12}
\begin{multline*}
\exp
\left\{
-\frac{1}{2} 
\left(
(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{aa}(\bm{x}_a - \bm{u}_a) + 2(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{ab}(\bm{x}_b - \bm{u}_b) 
\right.\right.\\
 + (\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{bb}(\bm{x}_b - \bm{u}_b) 
\\\left.\left.
+\frac{1}{2}(\bm{x}_b - \bm{\mu}_b)^\top \bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b)
\right)
\right\}
\end{multline*}

Again using Theorem \ref{thrm3}, 
\begin{multline*}
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}^{1}=
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}\\=
\begin{bmatrix}
(\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1} & -(\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}\bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1} \\
-\bm{\Sigma}_{bb}\bm{\Sigma}_{ba} (\bm{\Sigma}_{aa}- \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1} & \bm{\Sigma}_{bb}^{-1} + \bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} (\bm{\Sigma}_{aa} - \bm{\Lambda}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}\bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}
\end{bmatrix}
\end{multline*}

Plugging in,
\begin{multline*}
= \exp\left\{
-\frac{1}{2}
\left(
(\bm{x}_a - \bm{u}_a) ^\top (\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}(\bm{x}_a - \bm{u}_a) 
\right. \right. \\
- 2(\bm{x}_a - \bm{u}_a) ^\top(\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}\bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{u}_b) 
\\ \left.
+ (\bm{x}_b - \bm{u}_b) ^\top \bm{\Sigma}_{bb}^{-1} + \bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} (\bm{\Sigma}_{aa} - \bm{\Lambda}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}\bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{u}_b) \right)
\\  \left. 
+\frac{1}{2}(\bm{x}_b - \bm{\mu}_b)^\top \bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b)
\right\}
\end{multline*}

Multiplying out the third term in the exponent,
\begin{multline*}
= \exp
\Bigg\{
-\frac{1}{2}
\left(
(\bm{x}_a - \bm{u}_a) ^\top (\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}(\bm{x}_a - \bm{u}_a) 
 \right. 
\\
- 2(\bm{x}_a - \bm{u}_a) ^\top(\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}\bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{u}_b) 
\\ 
+ \uwave{(\bm{x}_b - \bm{u}_b) ^\top \bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{u}_b) }
\\ \left.
+ (\bm{x}_b - \bm{u}_b) ^\top \left[ \bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} (\bm{\Sigma}_{aa} - \bm{\Lambda}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1} \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\right](\bm{x}_b - \bm{u}_b) 
\right)
\\  \left. 
+\uwave{\frac{1}{2}(\bm{x}_b - \bm{\mu}_b)^\top \bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b)}
\right\}
\end{multline*}

The two underlined terms cancel (when the $-\frac{1}{2}$) gets distributed, and we are left with
\begin{multline}
\label{eq13}
= \exp
\left\{
-\frac{1}{2}
\left(
(\bm{x}_a - \bm{u}_a) ^\top (\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}(\bm{x}_a - \bm{u}_a) 
\right. \right. 
\\
- 2(\bm{x}_a - \bm{u}_a) ^\top(\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}\bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{u}_b) 
\\ \left. \left.
+ (\bm{x}_b - \bm{u}_b) ^\top \left[ \bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} (\bm{\Sigma}_{aa} - \bm{\Lambda}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1} \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\right](\bm{x}_b - \bm{u}_b) 
\right) \right.
\bigg\}
\end{multline}

In order apply another result from linear algebra, we need to summarize equation \eqref{eq13} using the following definitions. Set
\begin{equation}
\label{def1}
\begin{aligned}
 \bm{a} = (\bm{x}_a - \bm{\mu}_a) \\
\bm{d} = (\bm{x}_b - \bm{\mu}_b)
\end{aligned} \qquad\qquad
\begin{aligned}
\bm{B} &= (\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1}\\
\bm{C} &= \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\\
\bm{D} &= \bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba}
\end{aligned}
\end{equation}

We also note that
\begin{align*}
\bm{C} &= \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\\
\bm{C}^\top &= \left( \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1} \right)^\top\\
&=\bm{\Sigma}_{bb}^{-\top} \bm{\Sigma}_{ab}^{-1} \\
&\text{Transpose of symmetric matrix is that same matrix.}\\
&\text{And note $\bm{\Sigma}_{ab}^{-1}  = \bm{\Sigma}_{ba}$}\\
&=\bm{\Sigma}_{bb}^{-1} \bm{\Sigma}_{ba} = \bm{D}
\end{align*}

Rewriting equation \eqref{eq13} with these definitions \eqref{def1} and the replacement of $\bm{D}$ with $\bm{C}^\top$,
\begin{equation}
\label{eq14}
\exp\left\{ -\frac{1}{2} \left( \bm{a}^\top\bm{B}\bm{a} - 2\bm{a}^\top\bm{B}\bm{C}\bm{d} + \bm{d}\bm{C}^\top\bm{B}\bm{C}\bm{d} \right) \right\}
\end{equation}

Note that $\bm{C}\bm{d}$ is a vector. Using the following theorem from linear algebra.
\begin{theorem}
\label{thrm5}
For any vectors $\bm{u}$ and $\bm{v}$ and symmetric matrix $\bm{A}$:
\[
\bm{u}^\top\bm{A}\bm{u} - 2\bm{u}^\top\bm{A}\bm{v} + \bm{v}^\top\bm{A}\bm{v} = (\bm{u}^\top - \bm{v}^\top)\bm{A} (\bm{u} - \bm{v})
\]
\begin{proof}
\begin{align*}
\bm{u}^\top\bm{A}\bm{u} - 2\bm{u}^\top\bm{A}\bm{v} + \bm{v}^\top\bm{A}\bm{v} &= \bm{u}^\top\bm{A}\bm{u} -\bm{u}^\top\bm{A}\bm{v}-\bm{u}^\top\bm{A}\bm{v}  + \bm{v}^\top\bm{A}\bm{v}\\
&=\bm{u}^\top\bm{A}(\bm{u} - \bm{b}) - \bm{u}^\top\bm{A}\bm{v} + \bm{v}^\top\bm{A}\bm{v}\\
&=\bm{u}^\top\bm{A}(\bm{u} - \bm{b}) - (\bm{u}^\top - \bm{v}^\top)\bm{A}\bm{v}\\
&=\bm{u}^\top\bm{A}(\bm{u} - \bm{b}) - \underbrace{(\bm{u} - \bm{v})^\top\bm{A}\bm{v}}_{\text{transpose}}\\
&=\bm{u}^\top\bm{A}(\bm{u} - \bm{b}) - \bm{v}^\top\bm{A}(\bm{u} - \bm{v})\\
&=(\bm{u}^\top\bm{A} - \bm{v}^\top\bm{A}) (\bm{u} - \bm{v})\\
&=(\bm{u}^\top - \bm{v}^\top)\bm{A} (\bm{u} - \bm{v})
\end{align*}
\end{proof}
\end{theorem}

Call vector $\bm{C}\bm{d} = \bm{e}$ and rewrite equation \eqref{eq14},
\begin{equation*}
\exp\left\{ -\frac{1}{2} \left( \bm{a}^\top\bm{B}\bm{a} - 2\bm{a}^\top\bm{B}\bm{e} + \bm{d}\bm{C}^\top\bm{B}\bm{e} \right) \right\}
\end{equation*}

Using Theorem \ref{thrm5}, we have
\[
\exp\left\{ -\frac{1}{2} \left( \bm{a}^\top\bm{B}\bm{a} - 2\bm{a}^\top\bm{B}\bm{e} + \bm{d}\bm{C}^\top\bm{B}\bm{e} \right) \right\} = \exp\left\{ -\frac{1}{2}  (\bm{a}-\bm{e})^\top \bm{B} (\bm{a}-\bm{e})  \right\}
\]

Substitute back
\begin{align*}
&\begin{multlined}
\exp\left\{ -\frac{1}{2}  ((\bm{x}_a - \bm{\mu}_a)-\bm{C}\bm{d})^\top \bm{B} ((\bm{x}_a - \bm{\mu}_a)-\bm{C}\bm{d})  \right\}
\end{multlined} \\
=&\begin{multlined}
\exp\left\{ -\frac{1}{2}  ((\bm{x}_a - \bm{\mu}_a)-\bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b))^\top (\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1} 
\right.
\\ \left.
 ((\bm{x}_a - \bm{\mu}_a)-\bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b))  \right\} 
\end{multlined}
\end{align*}

Pull out the minus sign to get the equation into the form $(x - \mu)$:
\begin{multline}
\label{eq16}
\exp\left\{ -\frac{1}{2}  ((\bm{x}_a - (\bm{\mu}_a + \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b))^\top (\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1} 
\right.
\\ 
 ((\bm{x}_a - (\bm{\mu}_a + \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b))  \bigg\} 
\end{multline}

Equation \eqref{eq16} now looks like the pdf of a multivariate normal distribution with mean vector
\[
 \bm{\mu}_a + \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b)
\]

and covariance matrix
\[
\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} 
\]

All that is left is to deal with the normalizing constant we left up above.
\[
\frac{1}{(2\pi)^{\frac{d}{2}}} \cdot \frac{\det(\bm{\Sigma_{bb}})^{\frac{1}{2}}}{\det(\bm{\Sigma})^{\frac{1}{2}}}
\]

Using Theorem \ref{thrm4},
\[
\det(\bm{\Sigma}) = \det\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb} \\
\end{bmatrix} = \det(\bm{\Sigma}_{bb})\det(\bm{\Sigma}_{bb}-\bm{\Sigma}_{aa}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba})
\]
\begin{align}
\label{eq18}
&=\frac{1}{(2\pi)^{\frac{d}{2}}} \cdot \frac{\det(\bm{\Sigma}_{bb})^{\frac{1}{2}}}{\det(\bm{\Sigma}_{bb})^{\frac{1}{2}}\det(\bm{\Sigma}_{bb}-\bm{\Sigma}_{aa}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba})^{\frac{1}{2}}}\\
&=\frac{1}{(2\pi)^{\frac{d}{2}}} \cdot \frac{1}{\det(\bm{\Sigma}_{bb}-\bm{\Sigma}_{aa}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba})^{\frac{1}{2}}}
\end{align}

Putting equations \eqref{eq16} and \eqref{eq18} together, we get
\begin{multline}
p(\bm{x}_a \, | \, \bm{x}_b) = \\
\frac{1}{(2\pi)^{\frac{d}{2}}\det(\bm{\Sigma}_{bb}-\bm{\Sigma}_{aa}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba})^{\frac{1}{2}}} \cdot \\
\exp\left\{ -\frac{1}{2}  ((\bm{x}_a - (\bm{\mu}_a + \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b))^\top (\bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba} ) ^{-1} 
\right.
\\ 
 ((\bm{x}_a - (\bm{\mu}_a + \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b - \bm{\mu}_b))  \bigg\} 
\end{multline}

We can now read off the conditional mean vector and covariance matrix as
\[\bm{\mu}_a + \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\qquad\]
and
\[\bm{\Sigma}_{bb}-\bm{\Sigma}_{aa}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba}\qquad\square\]

Credit for parts of the proofs above goes to the following \cite{machinelearningsim}, \cite{statproofbook}, \cite{wang2006} and \cite{murphy2012machine}.

\printbibliography




\end{document}