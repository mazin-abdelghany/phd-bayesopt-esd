\documentclass{article}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{xfrac}

\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\title{Derivation of Multivariate Normal marginal and conditional distributions}
\author{Mazin Abdelghany, MD, MS}
\date{3 December 2025}

\begin{document}

\maketitle

\section{Background}

Gaussian process regression relies on several special properties of multivariate normal (MVN) distributions. The two most important of those properties are that:
\begin{enumerate}
\item The conditional distribution of a multivariate normal is normal
\item The marginal distribution of a multivariate normal is normal
\end{enumerate}

Moreover, these two above items can be calculated in closed form. Suppose that we have an $n$-dimensional Gaussian\footnote{Gaussian and normal will be interchanged throughout this document and used as synonyms.} random vector\footnote{Throughout this document, vectors will be denoted using lowercased bold letters while matrices will be denoted as uppercase bold letters.} $\bm{x}\in\mathbb{R}^n$. That is, 
\[
\bm{x} = \begin{bmatrix}
x_1 \sim \mathcal{N}(\mu_1, \sigma_1^2) \\
x_2 \sim \mathcal{N}(\mu_2, \sigma_2^2) \\
\vdots\\
x_n \sim \mathcal{N}(\mu_3, \sigma_3^2) 
\end{bmatrix}
\]

Using vector-matrix notation, we can rewrite the distribution statement as:
\[
\bm{x} \sim \mathcal{N}_n(\bm{\mu}, \bm{\Sigma})
\]

with the subscipt $n$ indicating that vector $\bm{x}$ is $n$-dimensional.

In this case, $\bm{\mu}$ has become a mean \textit{\textbf{vector}} of (possibly different) means for each of our random variables and $\bm{\Sigma}$ has become a covariace \textit{\textbf{matrix}}:
\begin{align*}
\bm{\mu} = \begin{bmatrix}
\mu_1\\
\mu_2\\
\vdots\\
\mu_n
\end{bmatrix} &&
\bm{\Sigma} = \begin{bmatrix}
\sigma_{11} & \sigma_{12} & \cdots & \sigma_{1n} \\
\sigma_{21} & \sigma_{22} & \cdots & \sigma_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{n1} & \sigma_{n2} & \cdots & \sigma_{nn} 
\end{bmatrix}
\end{align*}

We can make a few observations about this formulation:
\begin{enumerate}
\item $\bm{\mu} \in \mathbb{R}^n$ and $\bm{\Sigma}\in\mathbb{R}^{n\times n}$ (i.e., the mean is $n$-dimensional and the covariance matrix is $n\times n$-dimensional).
\item $\bm{\mu}$ is the same number of dimensions as $\bm{x}$.
\item $\bm{\mu}$ can assign a different $\mu_i$ for each $x_i \in \bm{x}$.
\item $\bm{\Sigma}$ is symmetric and all its entries are positive or zero (more strictly, it must be positive semidefinite).
\item The diagnoal of $\bm{\Sigma}$ is the covariance between $x_i$ and itself, which is the same as the variance of $x_i$ (i.e., $\sigma_{11} = \sigma_1^2$).
\end{enumerate}

The pdf of a multivariate Gaussian is:
\[
f(\bm{x}) = \frac{1}{ (2\pi)^{\sfrac{n}{2}} \det(\bm{\Sigma})^{\sfrac{1}{2}}  } \exp \left\{-\frac{1}{2} (\bm{x} - \bm{\mu})^\top \bm{\Sigma}^{-1} (\bm{x} - \bm{\mu})  \right\}
\]

Now, let us partition our multivariate normal random vector $\bm{x}\in\mathbb{R}^n$ into two multvariate normal random vectors $\bm{x}_a\in\mathbb{R}^d$ and $\bm{x}_b\in\mathbb{R}^{(n-d)}$:\footnote{Note the dimensions of each vector!}
\[
\bm{x} = \begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix}
\]

This new partitioned vector should have a multivariate normal distribution denoted by
\begin{equation}\label{a}
\begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix} = \mathcal{N}\left( \begin{bmatrix}
\bm{u}_a \\
\bm{u}_b
\end{bmatrix}, \begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix} \right)
\end{equation}

With this background in mind, we can proceed to show that

\begin{enumerate}
\item The conditional mean vector and covariance matrix of a multivariate normal for the vector $\bm{x}_a$ given a realization of the vector $\bm{x}_b$ are
\begin{itemize}
\item $\bm{\mu}_{\bm{x}_a\,|\,\bm{x}_b} = \bm{\mu_a} + \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}(\bm{x}_b-\bm{\mu}_b)$
\item $\bm{\Sigma}_{\bm{x}_a\,|\,\bm{x}_b} = \bm{\Sigma}_{aa} - \bm{\Sigma}_{ab}\bm{\Sigma}_{bb}^{-1}\bm{\Sigma}_{ba}$
\end{itemize}
\item The marginal mean vector and covariance matrix of a multivariate normal for $\bm{x}_a$ are
\begin{itemize}
\item $\bm{\mu}_{\bm{x}_a} = \bm{\mu}_a$
\item $\bm{\Sigma}_{\bm{x}_a} = \bm{\Sigma}_{aa}$
\end{itemize}
\end{enumerate}

\section{Marginal of mulitvariate normal}

Let us start with proof of the marginal mean vector and covariance matrix. The marginal pdf of $\bm{x}_a$ is:
\begin{align*}
f(\bm{x}_a) = \int_{\mathbb{R}^{(n-d)}} f(\bm{x}_a, \bm{x}_b)\, d\bm{x}_b
\end{align*}

because we are integrating out $\bm{x}_b$ leaving only the pdf of $\bm{x}_a$. Plugging in the values of the mean vectors and covariance matrices from equation \eqref{a}, we can write
\begin{align*}
f(\bm{x}_a) &= \int_{\mathbb{R}^{(n-d)}} \frac{1}{ (2\pi)^{\sfrac{n}{2}} \det(\bm{\Sigma})^{\sfrac{1}{2}}  } \exp \left\{-\frac{1}{2} \left(
\begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix} - 
\begin{bmatrix}
\bm{u}_a \\
\bm{u}_b
\end{bmatrix}\right)^\top 
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}^{-1} 
\left(\begin{bmatrix}
\bm{x}_a \\
\bm{x}_b
\end{bmatrix} - 
\begin{bmatrix}
\bm{u}_a \\
\bm{u}_b
\end{bmatrix}\right)  \right\}\, d\bm{x}_b \\
&= \int_{\mathbb{R}^{(n-d)}} \frac{1}{ (2\pi)^{\sfrac{n}{2}} \det(\bm{\Sigma})^{\sfrac{1}{2}}  } \exp \left\{-\frac{1}{2} \left(
\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}\right)^\top 
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}^{-1} 
\left(\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}\right)  \right\}\, d\bm{x}_b
\end{align*}

First, to simplify notation, let us define the normalizing constant as
\[
Q = (2\pi)^{\sfrac{n}{2}} \det(\bm{\Sigma})^{\sfrac{1}{2}}  
\]

Now, we need to introduce the concept of the precision matrix $\bm{\Lambda}$. The precision matrix is defined as the inverse of the covariance matrix: $\bm{\Lambda}=\bm{\Sigma}^{-1}$. Using this new definition, we note that
\[
\bm{\Lambda}=
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}=
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}^{-1}=
\bm{\Sigma}^{-1}
\]

Now we can rewrite the above as:
\begin{equation}
\label{eq2}
f(\bm{x}_a) = \int_{\mathbb{R}^{(n-d)}} \frac{1}{ Q } \exp \left\{-\frac{1}{2} \left(
\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}\right)^\top 
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}
\left(\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}\right)  \right\}\, d\bm{x}_b
\end{equation}

It is important to note that $\bm{\Lambda}_{aa} \neq \bm{\Sigma}_{aa}^{-1}$ but rather that $\bm{\Lambda}=\bm{\Sigma}^{-1}$!

Let us expand the positive of the expression within the exponent $\exp\{\cdot\}$ in two steps:
\[
\begin{bmatrix}
\bm{x}_a - \bm{u}_a  \\
\bm{x}_b - \bm{u}_b
\end{bmatrix}^\top 
\begin{bmatrix}
\bm{\Lambda}_{aa} (\bm{x}_a - \bm{u}_a) + \bm{\Lambda}_{ab} (\bm{x}_b - \bm{u}_b) \\
\bm{\Lambda}_{ba} (\bm{x}_a - \bm{u}_a) + \bm{\Lambda}_{ab} (\bm{x}_b - \bm{u}_b)
\end{bmatrix}
\]
\begin{multline}
\label{eq3}
\frac{1}{2}(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{aa}(\bm{x}_a - \bm{u}_a) + \frac{1}{2}(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{ab}(\bm{x}_b - \bm{u}_b) + \\
+ \frac{1}{2}(\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{ba}(\bm{x}_a - \bm{u}_a) + \frac{1}{2}(\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{bb}(\bm{x}_b - \bm{u}_b) 
\end{multline}

At this point we need to take advantage of the concept of completing the square. 
\begin{theorem}
\label{thrm1}
Let $\bm{A}\in\mathbb{R}^{n \times n}$ be a symmetric, positive definite matrix and $\{\bm{z}, \bm{b}, \bm{c}\} \in \mathbb{R}^n$, then
\[
\frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\bm{z} + \bm{c} = \frac{1}{2}(\bm{z} + \bm{A}^{-1}\bm{b})^\top\bm{A}(\bm{z}+\bm{A}^{-1}\bm{b})+\bm{c} - \frac{1}{2}\bm{b}^\top\bm{A}^{-1}\bm{b}
\] 
\end{theorem}
\begin{lemma}
Note that $\bm{b}^\top \bm{z}$ is a scalar ($\mathbb{R}^{1\times n} \cdot \mathbb{R}^{n \times 1} = 1 \times 1$). Note also that 
\begin{equation*}
\begin{aligned}
\bm{b}^\top \bm{z} &= \begin{bmatrix}
b_1 & b_2 & \dots & b_n
\end{bmatrix}\begin{bmatrix}
z_1 \\ z_2 \\ \dots \\ z_n
\end{bmatrix}\\
&=b_1z_1+b_2z_1+\dots+b_nz_n
\end{aligned}\qquad
\begin{aligned}
\bm{z}^\top \bm{b} &= \begin{bmatrix}
z_1 & z_2 & \dots & z_n
\end{bmatrix}\begin{bmatrix}
b_1 \\ b_2 \\ \dots \\ b_n
\end{bmatrix}\\
&=z_1b_1+z_1b_2+\dots+z_nb_n
\end{aligned}
\end{equation*}
By the commutative property of multiplication of scalars, $\bm{b}^\top \bm{z} = \bm{z}^\top \bm{b}$.
\end{lemma}
\begin{proof}
Thus,
\[
\frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\bm{z} + \bm{c} = \frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \underbrace{\frac{1}{2}\bm{b}^\top\bm{z} + \frac{1}{2}\bm{z}^\top\bm{b}}_{\text{from lemma}} + \bm{c}
\]
Then, add and subtract $\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}$
\begin{align*}
&= \frac{1}{2} \bm{z}^\top \bm{A}\bm{z} +\frac{1}{2}\bm{b}^\top\bm{z} + \frac{1}{2}\bm{z}^\top\bm{b} + \bm{c} + \left(\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\right)\\
&\text{Rearrange terms.}\\
&= \frac{1}{2} \bm{b}^\top\bm{z} + \frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b} + \frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \frac{1}{2}\bm{z}^\top\bm{b} + \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Pull out the $\frac{1}{2}$.}\\
&= \frac{1}{2} (\bm{b}^\top\bm{z} + \bm{b}^\top \bm{A}^{-1}\bm{b} + \bm{z}^\top \bm{A}\bm{z} +\bm{z}^\top\bm{b} )+ \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Rearrange terms.}\\
&= \frac{1}{2} (\bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\bm{z}  +\bm{z}^\top\bm{b} + \bm{b}^\top \bm{A}^{-1}\bm{b})+ \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
\end{align*}
We can now make a comparison to the scalar version of completing the square:
\[
=\frac{1}{2} (\underbrace{\bm{z}^\top \bm{A}\bm{z}}_{\text{like $Ax^2$}} + 
\underbrace{\bm{b}^\top\bm{z}  +\bm{z}^\top\bm{b}}_{\text{like $2zb$}} + 
\underbrace{\bm{b}^\top \bm{A}^{-1}\bm{b}}_{\text{like $\frac{1}{A}b^2$}})
+ \bm{c} - \frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}
\]
In order to factorize the above in parentheses, we need to fill in some blanks:
\[
\frac{1}{2} (\bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\text{\underline{\phantom{xx}} }\bm{z}  +\bm{z}^\top\text{\underline{\phantom{xx}} }\bm{b} + \bm{b}^\top \bm{A}^{-1}\text{\underline{\phantom{xx}} }\bm{b})+ \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}
\]
To leave the equation the same, we multiply by the identity matrix: $\bm{I} = \bm{A}\bm{A}^{-1}$.
\begin{align*}
&=\frac{1}{2} (\bm{z}^\top \bm{A}\bm{z} +\bm{z}^\top\bm{A}\bm{A}^{-1}\bm{b} + \bm{b}^\top\bm{A}^{-1}\bm{A}\bm{z}   + \bm{b}^\top \bm{A}^{-1}\bm{A}\bm{A}^{-1}\bm{b})+ \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Pull out $(\bm{z} + \bm{A}^{-1}\bm{b})$ on the right.}\\
&=\frac{1}{2} ( \bm{z}^\top \bm{A} + \bm{b}^\top\bm{A}^{-1}\bm{A})(\bm{z} + \bm{A}^{-1}\bm{b} ) + \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Pull out $\bm{A}$ from the first expression.}\\
&=\frac{1}{2} ( \bm{z}^\top + \bm{b}^\top\bm{A}^{-1})\bm{A}(\bm{z} + \bm{A}^{-1}\bm{b} ) + \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
&\text{Remove a transpose from the first expression.}\\
\frac{1}{2} \bm{z}^\top \bm{A}\bm{z} + \bm{b}^\top\bm{z} + \bm{c}&=\frac{1}{2} ( \bm{z} + \bm{A}^{-1}\bm{b})^\top\bm{A}(\bm{z} + \bm{A}^{-1}\bm{b} ) + \bm{c}-\frac{1}{2} \bm{b}^\top \bm{A}^{-1}\bm{b}\\
\end{align*}
\end{proof}

Back to the problem at hand. Let us complete the square from equation \eqref{eq3}:
\begin{multline*}
\frac{1}{2}(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{aa}(\bm{x}_a - \bm{u}_a) + \frac{1}{2}(\bm{x}_a - \bm{u}_a) ^\top \bm{\Lambda}_{ab}(\bm{x}_b - \bm{u}_b) + \\
+ \frac{1}{2}(\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{ba}(\bm{x}_a - \bm{u}_a) + \frac{1}{2}(\bm{x}_b - \bm{u}_b) ^\top \bm{\Lambda}_{bb}(\bm{x}_b - \bm{u}_b) 
\end{multline*}

Define the following recalling that all precision and covariance matrices are positive semidefinite:
\begin{equation*}
\begin{aligned}
\bm{A} = \bm{\Lambda}_{bb}
\end{aligned} \qquad
\begin{aligned}
\bm{z} &= \bm{x}_b - \bm{\mu}_b\\
\bm{b} &= \bm{\Lambda}_{ba} (\bm{x}_b - \bm{\mu}_b)\\
\bm{c} &= \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa} (\bm{x}_a - \bm{\mu}_a)
\end{aligned}
\end{equation*}

Note also that, using the fact that $\bm{\Lambda}_{ba}^\top = \bm{\Lambda}_{ab}$,
\[
\bm{b} = \bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \implies \bm{b}^\top =  (\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}
\]

We note,
\begin{align*}
\frac{1}{2} \bm{z}^\top \bm{A}\bm{z} &= \frac{1}{2} (\bm{x}_b - \bm{\mu}_b)^\top\bm{\Lambda}_{bb}(\bm{x}_b - \bm{\mu}_b) \\
\bm{b}^\top\bm{z} &= (\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}(\bm{x}_b - \bm{\mu}_b)\\
\bm{c} &=  \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa} (\bm{x}_a - \bm{\mu}_a)
\end{align*}

And applying Theorem \eqref{thrm1},
\begin{multline}
\label{eq4}
\frac{1}{2} \Big[ \big(  (\bm{x}_b - \bm{\mu}_b) +\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)^\top \bm{\Lambda}_{bb}  \big(  (\bm{x}_b - \bm{\mu}_b) + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)\Big] \\
+ \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) - \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a)
\end{multline}

Keeping track of the $-1$ that we left out when completing the square, and combining \eqref{eq4} with \eqref{eq2}, the full expression becomes:
\begin{multline*}
f(\bm{x}_a) = \frac{1}{Q} \int_{\mathbb{R}^{n-d}} \exp \left\{ -\frac{1}{2} \Big[ \big(  (\bm{x}_b - \bm{\mu}_b) +\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)^\top \bm{\Lambda}_{bb}  \big(  (\bm{x}_b - \bm{\mu}_b) +\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)\Big] \right. \\
\left. - \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) + \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a) \right\}\, d\bm{x}_b
\end{multline*}

Since we are integrating over $\bm{x}_b$, all of the terms that contain $\bm{x}_a$ can be taken outside of the integral. Using $e^{ab} = e^ae^b$,
\begin{multline}
\label{eq5}
f(\bm{x}_a) = \frac{1}{Q} \exp \left\{-\frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) + \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a) \right\}\\ \int_{\mathbb{R}^{n-d}} \exp \left\{ -\frac{1}{2} \Big[ \big(  (\bm{x}_b - \bm{\mu}_b) + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)^\top \bm{\Lambda}_{bb}  \big(  (\bm{x}_b - \bm{\mu}_b) +\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) \big)\Big]
   \right\}\, d\bm{x}_b
\end{multline}

We can remove the inner parentheses $ (\bm{x}_b - \bm{\mu}_b) + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) $ and instead write $\bm{x}_b - \bm{\mu}_b + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a)$.

We now observe that if we replace $ \bm{\mu}_b + \bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba} (\bm{x}_a - \bm{\mu}_a) $ with $\bm{m}$, the above integral in equation \eqref{eq5} becomes
\[
\int_{\mathbb{R}^{n-d}} \exp \left\{ -\frac{1}{2} \Big[ \big(  (\bm{x}_b - \bm{m} \big)^\top \bm{\Lambda}_{bb}  \big(  (\bm{x}_b - \bm{m} \big)\Big]
   \right\}\, d\bm{x}_b
\]

Note, that this integral is a multivariate normal kernel with mean vector $\bm{m}$ and covariance matrix $\bm{\Lambda}_{bb}$ and that we are integrating over the correct dimensionality of $\bm{x}_b$! Therefore, the integral evaluates to the normalizing constant
\[
(2\pi)^{\sfrac{n-d}{2}} \det(\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}}
\]

Plugging this into equation \eqref{eq5}, we write
\begin{multline*}
f(\bm{x}_a) = \frac{1}{Q} (2\pi)^{\sfrac{n-d}{2}} \det(\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}}\\  \exp \left\{\frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) + \frac{1}{2}(\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a) \right\}
\end{multline*}

Pulling out a $-\frac{1}{2}$, 
\begin{multline*}
f(\bm{x}_a) = \frac{1}{Q} (2\pi)^{\sfrac{n-d}{2}} \det(\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}}\\  \exp \left\{-\frac{1}{2}\left[ (\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{aa}(\bm{x}_a - \bm{\mu}_a) - (\bm{x}_a - \bm{\mu}_a)^\top\bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a)\right]   \right\}
\end{multline*}

Then pulling out a $(\bm{x}_a - \bm{\mu}_a)^\top$ from the left and an $(\bm{x}_a - \bm{\mu}_a)$ from the right, we get
\begin{multline}
\label{eq6}
f(\bm{x}_a) = \frac{1}{Q} (2\pi)^{\sfrac{n-d}{2}} \det(\bm{\Lambda}_{bb}^{-1})^{\sfrac{1}{2}}\\  \exp \left\{-\frac{1}{2}\left[ (\bm{x}_a - \bm{\mu}_a)^\top \bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}(\bm{x}_a - \bm{\mu}_a)\right]   \right\}
\end{multline}

Equation \eqref{eq6} appears to be a MVN with mean vector $\bm{\mu}_a$ and covariance matrix $\bm{\Lambda}_{aa} - \bm{\Lambda}_{ab}\bm{\Lambda}_{bb}^{-1}\bm{\Lambda}_{ba}$. Now, all that is left is to rewrite the covariance matrix and the normalization constant in terms of $\bm{\Sigma}$.

Recall,
\[
\bm{\Lambda}=
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}=
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}^{-1}=
\bm{\Sigma}^{-1}
\]

Thus, $\bm{\Lambda}^{-1} = \bm{\Sigma}$ and
\[
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}^{-1}=
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}
\]

We now need a result from linear algebra.

\begin{theorem}
\label{thrm3}
For a block matrix 
\[
\bm{M} = \begin{bmatrix}
\bm{A} & \bm{B} \\
\bm{C} & \bm{D}
\end{bmatrix}
\]
where $\bm{M}$ is symmetric, positive definite, we can define $\bm{M}^{-1}$ using the Shur complement of $\bm{D}$ in $\bm{M}$, $(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1}$:
\[
\begin{bmatrix}
\bm{A} & \bm{B} \\
\bm{C} & \bm{D}
\end{bmatrix}^{-1} = \begin{bmatrix}
(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1} & -(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1}\bm{B}\bm{D}^{-1} \\
-\bm{D}\bm{C}(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1} & \bm{D}^{-1} + \bm{D}^{-1}\bm{C}(\bm{A} - \bm{B}\bm{D}^{-1}\bm{C}) ^{-1}\bm{B}\bm{D}^{-1}
\end{bmatrix}
\]
\end{theorem}

Using Theorem \eqref{thrm3}, we write
\[
\begin{bmatrix}
\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab} \\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{bmatrix}=
\begin{bmatrix}
\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab} \\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{bmatrix}^{-1}=
\begin{bmatrix}
 &  \\
 & 
\end{bmatrix}
\]

\section{Conditional of multivariate normal}






















































\end{document}